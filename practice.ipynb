{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install\n",
    "pip install 'transformers==2.1.1'\n",
    "pip install sklearn\n",
    "pip install tensorboardX\n",
    "pip install torch\n",
    "cd /home/ec2-user/SageMaker/projects/transformers\n",
    "pip install -r ./examples/requirements.txt\n",
    "cd /home/ec2-user/SageMaker/projects/transformer_entity_rel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install\n",
    "!pip install transformers\n",
    "!pip install sklearn\n",
    "!pip install tensorboardX\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/projects/transformers\n"
     ]
    }
   ],
   "source": [
    "cd /home/ec2-user/SageMaker/projects/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboardX in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r ./examples/requirements.txt (line 1)) (1.9)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r ./examples/requirements.txt (line 2)) (0.20.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.11.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.14.3)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (3.10.0)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn->-r ./examples/requirements.txt (line 2)) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorboardX->-r ./examples/requirements.txt (line 1)) (39.1.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ./examples/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/projects/transformer_entity_rel\n"
     ]
    }
   ],
   "source": [
    "cd /home/ec2-user/SageMaker/projects/transformer_entity_rel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import *\n",
    "import run_glue_gap\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "3000\n",
      "  targets                                              texts\n",
      "0       1  Glancy Prongay & Murray LLP Announces Investig...\n",
      "1       1  The case has been assigned to Judge Keith P. E...\n",
      "2       1  . It comes afterDavid Bolt, who provides indep...\n",
      "3       1  ROSEN , A GLOBALLY RECOGNIZED LAW FIRM , Remin...\n",
      "4       1  <BEGIN-SPAN> wells fargo wells fargo <END-SPAN...\n",
      "(303, 2)\n"
     ]
    }
   ],
   "source": [
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_df1.2.json'\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_df1.json'\n",
    "filename='/home/ec2-user/SageMaker/Data/contexts_samples_original1.json'\n",
    "\n",
    "#dict_keys(['query_name', 'contexts', 'article_title', 'conservative-marked-body', \n",
    "    #'conservative-sequence', 'liberal-marked-body', 'entity_relevance', 'article_body', 'liberal-sequence'])\n",
    "with open(filename, 'r') as f:\n",
    "            the_dic = json.loads(f.read())\n",
    "\n",
    "\n",
    "the_targets=[docca['entity_relevance'] for docca in the_dic]\n",
    "the_inputs=[]\n",
    "for docca in the_dic:\n",
    "    the_inputs.append(\" \".join(docca['sequence'].split()[:512]))\n",
    "    #the_inputs.append(\" \".join(docca['sequence'].split()[:250]))\n",
    "\n",
    "\n",
    "print(len(the_targets))\n",
    "print(len(the_inputs))\n",
    "the_data = pd.DataFrame(list(zip(the_targets, the_inputs)), columns =['targets', 'texts']) \n",
    "print(the_data.head())\n",
    "msk = np.random.rand(len(the_data)) < 0.9\n",
    "train_data = the_data[msk]\n",
    "test_data = the_data[~msk]\n",
    "print(np.shape(test_data))\n",
    "train_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data/train.tsv',sep='\\t',index=False,header=False)\n",
    "test_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data/dev.tsv',sep='\\t',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create data - OLD\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import run_glue_gap\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_original5.json'\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_MT_train_no_overlap.json'\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_MT_masked_train.json'\n",
    "filename='/home/ec2-user/SageMaker/Data/contexts_samples_MT_train_new.json'\n",
    "\n",
    "#dict_keys(['query_name', 'contexts', 'article_title', 'conservative-marked-body', \n",
    "    #'conservative-sequence', 'liberal-marked-body', 'entity_relevance', 'article_body', 'liberal-sequence'])\n",
    "with open(filename, 'r') as f:\n",
    "            the_dic = json.loads(f.read())\n",
    "\n",
    "libandcon_cases=[]\n",
    "libnotcon_cases=[]\n",
    "atleastone_cases=[]\n",
    "fail_cases=[]\n",
    "#the_targets=[]\n",
    "the_targets=[docca['entity_relevance'] for docca in the_dic]\n",
    "conservative_inputs=[]\n",
    "liberal_inputs=[]\n",
    "for doc_ind, docca in enumerate(the_dic):\n",
    "    con_body=docca['conservative-marked-body'].replace('\\n','')\n",
    "    con_seq=docca['conservative-sequence'].replace('\\n','')\n",
    "    lib_body=docca['liberal-marked-body'].replace('\\n','')\n",
    "    lib_seq=docca['liberal-sequence'].replace('\\n','')\n",
    "    if len(lib_seq)>0 and len(lib_body)>0: # if both lib_seq and lib_body not empty\n",
    "        if len(lib_body.split())<len(lib_seq.split()): #choose the shorter bw lib_body and lib_seq to append to lib\n",
    "            liberal_inputs.append(lib_body)\n",
    "        else:\n",
    "            liberal_inputs.append(lib_seq)\n",
    "        if len(con_seq)>0 and len(con_body)>0: # if con_seq and con_body also not empty\n",
    "            libandcon_cases.append(doc_ind)\n",
    "            if len(con_body.split())<len(con_seq.split()): #choose the shorter bw con_body and con_seq to append to con\n",
    "                conservative_inputs.append(con_body)\n",
    "            else:\n",
    "                conservative_inputs.append(con_seq)\n",
    "        else: #lib not empty but con empty\n",
    "            libnotcon_cases.append(doc_ind)\n",
    "            conservative_inputs.append(liberal_inputs[-1])        \n",
    "        #the_targets.append(docca['entity_relevance'])\n",
    "    elif len(lib_seq)>0 or len(lib_body)>0 or len(con_seq)>0 or len(con_body)>0:\n",
    "        atleastone_cases.append(doc_ind)\n",
    "        if len(con_seq)>0:\n",
    "            conservative_inputs.append(con_seq)\n",
    "            liberal_inputs.append(con_seq)\n",
    "        elif len(con_body)>0:\n",
    "            conservative_inputs.append(con_body)\n",
    "            liberal_inputs.append(con_body)\n",
    "        elif len(lib_seq)>0:\n",
    "            conservative_inputs.append(lib_seq)\n",
    "            liberal_inputs.append(lib_seq)\n",
    "        elif len(lib_body)>0:\n",
    "            conservative_inputs.append(lib_body)\n",
    "            liberal_inputs.append(lib_body)\n",
    "        #the_targets.append(docca['entity_relevance'])\n",
    "    else:\n",
    "        #the_targets.append('0')\n",
    "        conservative_inputs.append('asdfasdfasdfasdf')\n",
    "        liberal_inputs.append('asdfasdfasdfasdf')\n",
    "        fail_cases.append(doc_ind)\n",
    "\n",
    "print(len(libandcon_cases))\n",
    "print(len(libnotcon_cases))\n",
    "print(len(atleastone_cases))\n",
    "print(fail_cases)\n",
    "print(len(the_targets))\n",
    "print(len(conservative_inputs))\n",
    "print(len(liberal_inputs))\n",
    "con_data = pd.DataFrame(list(zip(the_targets, conservative_inputs)), columns =['targets', 'texts']) \n",
    "lib_data = pd.DataFrame(list(zip(the_targets, liberal_inputs)), columns =['targets', 'texts']) \n",
    "con_data = con_data.sample(frac=1).reset_index(drop=True)#shuffle\n",
    "lib_data = lib_data.sample(frac=1).reset_index(drop=True)#shuffle\n",
    "print(con_data.head())\n",
    "print(lib_data.head())\n",
    "#con_data\n",
    "msk = np.random.rand(len(con_data)) < 0.9\n",
    "con_train_data = con_data[msk].reset_index(drop=True)\n",
    "con_test_data = con_data[~msk].reset_index(drop=True)\n",
    "print(con_test_data.head())\n",
    "print(np.shape(con_test_data))\n",
    "print(np.shape(con_train_data))\n",
    "con_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_con/orig_data.tsv',sep='\\t',index=False,header=False)\n",
    "con_train_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_con/train.tsv',sep='\\t',index=False,header=False)\n",
    "con_test_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_con/dev.tsv',sep='\\t',index=False,header=False)\n",
    "#lib_data\n",
    "msk = np.random.rand(len(lib_data)) < 0.9\n",
    "lib_train_data = lib_data[msk].reset_index(drop=True)\n",
    "lib_test_data = lib_data[~msk].reset_index(drop=True)\n",
    "print(lib_test_data.head())\n",
    "print(np.shape(lib_test_data))\n",
    "print(np.shape(lib_train_data))\n",
    "lib_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_lib/orig_data.tsv',sep='\\t',index=False,header=False)\n",
    "lib_train_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_lib/train.tsv',sep='\\t',index=False,header=False)\n",
    "lib_test_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_lib/dev.tsv',sep='\\t',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create data \n",
    "\n",
    "# For contexts_samples_MT_train_new:\n",
    "# #  <BEGIN-SPAN> Kyocera <END-SPAN>  is the tag\n",
    "# 'conservative-marked-body', \n",
    "# 'conservative-sequence',\n",
    "# 'liberal-marked-body', \n",
    "# 'liberal-sequence', \n",
    "# # Kyocera is the tag, (i.e. without <BEGIN-SPAN> and <END-SPAN> markers)\n",
    "# 'conservative-sequence-nospan', \n",
    "# 'liberal-sequence-nospan', \n",
    "# 'liberal-marked-body-nospan'\n",
    "# 'conservative-marked-body-nospan', \n",
    "# # <BEGIN-SPAN> ORGANIZATION1 <END-SPAN> is the tag\n",
    "# 'conservative-org-marked-body', \n",
    "# 'liberal-org-sequence', \n",
    "# 'liberal-org-marked-body', \n",
    "# 'conservative-org-sequence',\n",
    "# # ORGANIZATION1 is the tag, (i.e. without <BEGIN-SPAN> and <END-SPAN> markers)\n",
    "# 'conservative-org-marked-body-nospan'\n",
    "# 'conservative-org-sequence-nospan', \n",
    "# 'liberal-org-sequence-nospan', \n",
    "# 'liberal-org-marked-body-nospan', \n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import run_glue_gap\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_original5.json'\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_MT_train_no_overlap.json'\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_MT_masked_train.json'\n",
    "filename='/home/ec2-user/SageMaker/Data/contexts_samples_MT_train_new.json'\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_MT_train_normalized.json' #that's the wrong dataset. that's the set where Bruce used flare to replace all entities. not the same set at all\n",
    "\n",
    "#dict_keys(['query_name', 'contexts', 'article_title', 'conservative-marked-body', \n",
    "    #'conservative-sequence', 'liberal-marked-body', 'entity_relevance', 'article_body', 'liberal-sequence'])\n",
    "with open(filename, 'r') as f:\n",
    "            the_dic = json.loads(f.read())\n",
    "\n",
    "libandcon_cases=[]\n",
    "libnotcon_cases=[]\n",
    "connotlib_cases=[]\n",
    "fail_cases=[]\n",
    "#the_targets=[]\n",
    "the_targets=[docca['entity_relevance'] for docca in the_dic]\n",
    "conservative_inputs=[]\n",
    "liberal_inputs=[]\n",
    "for doc_ind, docca in enumerate(the_dic):\n",
    "    # for NORMAL e.g. <BEGIN-SPAN> Kyocera <END-SPAN>  is the tag\n",
    "    con_body=docca['conservative-marked-body'].replace('\\n','')\n",
    "    con_seq=docca['conservative-sequence'].replace('\\n','')\n",
    "    lib_body=docca['liberal-marked-body'].replace('\\n','')\n",
    "    lib_seq=docca['liberal-sequence'].replace('\\n','')\n",
    "#     # for e.g. Kyocera is the tag, (i.e. without <BEGIN-SPAN> and <END-SPAN> markers)\n",
    "#     con_body=docca['conservative-marked-body-nospan'].replace('\\n','')\n",
    "#     con_seq=docca['conservative-sequence-nospan'].replace('\\n','')\n",
    "#     lib_body=docca['liberal-marked-body-nospan'].replace('\\n','')\n",
    "#     lib_seq=docca['liberal-sequence-nospan'].replace('\\n','')\n",
    "#     # for e.g. <BEGIN-SPAN> ORGANIZATION1 <END-SPAN> is the tag\n",
    "#     con_body=docca['conservative-org-marked-body'].replace('\\n','')\n",
    "#     con_seq=docca['conservative-org-sequence'].replace('\\n','')\n",
    "#     lib_body=docca['liberal-org-marked-body'].replace('\\n','')\n",
    "#     lib_seq=docca['liberal-org-sequence'].replace('\\n','')\n",
    "#     # for e.g. ORGANIZATION1 is the tag, (i.e. without <BEGIN-SPAN> and <END-SPAN> markers)\n",
    "#     con_body=docca['conservative-org-marked-body-nospan'].replace('\\n','')\n",
    "#     con_seq=docca['conservative-org-sequence-nospan'].replace('\\n','')\n",
    "#     lib_body=docca['liberal-org-marked-body-nospan'].replace('\\n','')\n",
    "#     lib_seq=docca['liberal-org-sequence-nospan'].replace('\\n','')\n",
    "    \n",
    "    lib_choice=''\n",
    "    con_choice=''\n",
    "    \n",
    "    if len(lib_seq)>0 and len(lib_body)>0: # if both lib_seq and lib_body not empty\n",
    "        if len(lib_body.split())<len(lib_seq.split()): #choose the shorter bw lib_body and lib_seq to append to lib\n",
    "            lib_choice=lib_body\n",
    "        else:\n",
    "            lib_choice=lib_seq\n",
    "    else: #if only one or neither are not empty\n",
    "        lib_choice=lib_seq if len(lib_seq)>0 else lib_body if len(lib_body)>0 else lib_choice            \n",
    "    if len(con_seq)>0 and len(con_body)>0: # if con_seq and con_body not empty\n",
    "        if len(con_body.split())<len(con_seq.split()): #choose the shorter bw con_body and con_seq to append to con\n",
    "            con_choice=con_body\n",
    "        else:\n",
    "            con_choice=con_seq\n",
    "    else: #if only one or neither are not empty\n",
    "        con_choice=con_seq if len(con_seq)>0 else con_body if len(con_body)>0 else con_choice  \n",
    "    if len(con_choice)>0 and len(lib_choice)>0:\n",
    "        libandcon_cases.append(doc_ind)\n",
    "    if len(con_choice)==0: \n",
    "        con_choice=lib_choice\n",
    "        if len(lib_choice)>0:\n",
    "            libnotcon_cases.append(doc_ind)\n",
    "    if len(lib_choice)==0:\n",
    "        lib_choice=con_choice\n",
    "        if len(con_choice)>0:\n",
    "            connotlib_cases.append(doc_ind)\n",
    "    if len(con_choice)==0 and len(lib_choice)==0:\n",
    "        con_choice='asdfasdfasdfasdf'\n",
    "        lib_choice='asdfasdfasdfasdf'\n",
    "        fail_cases.append(doc_ind)\n",
    "    conservative_inputs.append(con_choice)\n",
    "    liberal_inputs.append(lib_choice)\n",
    "\n",
    "\n",
    "print('Len libandcon_cases: ' + str(len(libandcon_cases)))\n",
    "print('Len libnotcon_cases: ' + str(len(libnotcon_cases)))\n",
    "print('Len connotlib_cases: ' + str(len(connotlib_cases)))\n",
    "print('Len fail_cases: ' + str(len(fail_cases)))\n",
    "print(fail_cases)\n",
    "print('Sum: ' + str(len(libandcon_cases)+len(libnotcon_cases)+len(connotlib_cases)+len(fail_cases)))\n",
    "print(len(the_targets))\n",
    "print('Len conservative: ' + str(len(conservative_inputs)))\n",
    "print('Len liberal: ' + str(len(liberal_inputs)))\n",
    "con_data = pd.DataFrame(list(zip(the_targets, conservative_inputs)), columns =['targets', 'texts']) \n",
    "lib_data = pd.DataFrame(list(zip(the_targets, liberal_inputs)), columns =['targets', 'texts']) \n",
    "con_data = con_data.sample(frac=1).reset_index(drop=True)#shuffle\n",
    "lib_data = lib_data.sample(frac=1).reset_index(drop=True)#shuffle\n",
    "print(con_data.head())\n",
    "print(lib_data.head())\n",
    "#con_data\n",
    "msk = np.random.rand(len(con_data)) < 0.9\n",
    "con_train_data = con_data[msk].reset_index(drop=True)\n",
    "con_test_data = con_data[~msk].reset_index(drop=True)\n",
    "print(con_test_data.head())\n",
    "print(np.shape(con_test_data))\n",
    "print(np.shape(con_train_data))\n",
    "con_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_con/orig_data.tsv',sep='\\t',index=False,header=False)\n",
    "con_train_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_con/train.tsv',sep='\\t',index=False,header=False)\n",
    "con_test_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_con/dev.tsv',sep='\\t',index=False,header=False)\n",
    "# con_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_cheat2/orig_data.tsv',sep='\\t',index=False,header=False)\n",
    "# con_train_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_cheat2/train.tsv',sep='\\t',index=False,header=False)\n",
    "# con_test_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_cheat2/dev.tsv',sep='\\t',index=False,header=False)\n",
    "#lib_data\n",
    "msk = np.random.rand(len(lib_data)) < 0.9\n",
    "lib_train_data = lib_data[msk].reset_index(drop=True)\n",
    "lib_test_data = lib_data[~msk].reset_index(drop=True)\n",
    "print(lib_test_data.head())\n",
    "print(np.shape(lib_test_data))\n",
    "print(np.shape(lib_train_data))\n",
    "lib_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_lib/orig_data.tsv',sep='\\t',index=False,header=False)\n",
    "lib_train_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_lib/train.tsv',sep='\\t',index=False,header=False)\n",
    "lib_test_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_lib/dev.tsv',sep='\\t',index=False,header=False)\n",
    "print(the_dic[0].keys()) #Make sure these are the correct fields I'm using!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create data - CHEAT\n",
    "\n",
    "# After ran this for each of 1 2 3 4, copy the dev.tsv into each from entrel_data_con_eval_aetna_new#\n",
    "# cd /home/ec2-user/SageMaker/Data/\n",
    "# cp ./entrel_data_con_eval_aetna_new1/dev.tsv ./entrel_data_cheat1/\n",
    "# cp ./entrel_data_con_eval_aetna_new2/dev.tsv ./entrel_data_cheat2/\n",
    "# cp ./entrel_data_con_eval_aetna_new3/dev.tsv ./entrel_data_cheat3/\n",
    "# cp ./entrel_data_con_eval_aetna_new4/dev.tsv ./entrel_data_cheat4/\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import run_glue_gap\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_original5.json'\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_MT_train_no_overlap.json'\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_MT_masked_train.json'\n",
    "filename='/home/ec2-user/SageMaker/Data/contexts_samples_MT_train_new.json'\n",
    "\n",
    "#dict_keys(['query_name', 'contexts', 'article_title', 'conservative-marked-body', \n",
    "    #'conservative-sequence', 'liberal-marked-body', 'entity_relevance', 'article_body', 'liberal-sequence'])\n",
    "with open(filename, 'r') as f:\n",
    "            the_dic = json.loads(f.read())\n",
    "\n",
    "libandcon_cases=[]\n",
    "libnotcon_cases=[]\n",
    "connotlib_cases=[]\n",
    "fail_cases=[]\n",
    "#the_targets=[]\n",
    "the_targets=[docca['entity_relevance'] for docca in the_dic]\n",
    "conservative_inputs=[]\n",
    "liberal_inputs=[]\n",
    "for doc_ind, docca in enumerate(the_dic):\n",
    "    # 1 for NORMAL e.g. <BEGIN-SPAN> Kyocera <END-SPAN>  is the tag\n",
    "#     con_body=docca['conservative-marked-body'].replace('\\n','')\n",
    "#     con_seq=docca['conservative-sequence'].replace('\\n','')\n",
    "#     lib_body=docca['liberal-marked-body'].replace('\\n','')\n",
    "#     lib_seq=docca['liberal-sequence'].replace('\\n','')\n",
    "#     # 2 for e.g. Kyocera is the tag, (i.e. without <BEGIN-SPAN> and <END-SPAN> markers)\n",
    "    con_body=docca['conservative-marked-body-nospan'].replace('\\n','')\n",
    "    con_seq=docca['conservative-sequence-nospan'].replace('\\n','')\n",
    "    lib_body=docca['liberal-marked-body-nospan'].replace('\\n','')\n",
    "    lib_seq=docca['liberal-sequence-nospan'].replace('\\n','')\n",
    "#     # 3 for e.g. <BEGIN-SPAN> ORGANIZATION1 <END-SPAN> is the tag\n",
    "#     con_body=docca['conservative-org-marked-body'].replace('\\n','')\n",
    "#     con_seq=docca['conservative-org-sequence'].replace('\\n','')\n",
    "#     lib_body=docca['liberal-org-marked-body'].replace('\\n','')\n",
    "#     lib_seq=docca['liberal-org-sequence'].replace('\\n','')\n",
    "#     # 4 for e.g. ORGANIZATION1 is the tag, (i.e. without <BEGIN-SPAN> and <END-SPAN> markers)\n",
    "#     con_body=docca['conservative-org-marked-body-nospan'].replace('\\n','')\n",
    "#     con_seq=docca['conservative-org-sequence-nospan'].replace('\\n','')\n",
    "#     lib_body=docca['liberal-org-marked-body-nospan'].replace('\\n','')\n",
    "#     lib_seq=docca['liberal-org-sequence-nospan'].replace('\\n','')\n",
    "    \n",
    "    lib_choice=''\n",
    "    con_choice=''\n",
    "    \n",
    "    if len(lib_seq)>0 and len(lib_body)>0: # if both lib_seq and lib_body not empty\n",
    "        if len(lib_body.split())<len(lib_seq.split()): #choose the shorter bw lib_body and lib_seq to append to lib\n",
    "            lib_choice=lib_body\n",
    "        else:\n",
    "            lib_choice=lib_seq\n",
    "    else: #if only one or neither are not empty\n",
    "        lib_choice=lib_seq if len(lib_seq)>0 else lib_body if len(lib_body)>0 else lib_choice            \n",
    "    if len(con_seq)>0 and len(con_body)>0: # if con_seq and con_body not empty\n",
    "        if len(con_body.split())<len(con_seq.split()): #choose the shorter bw con_body and con_seq to append to con\n",
    "            con_choice=con_body\n",
    "        else:\n",
    "            con_choice=con_seq\n",
    "    else: #if only one or neither are not empty\n",
    "        con_choice=con_seq if len(con_seq)>0 else con_body if len(con_body)>0 else con_choice  \n",
    "    if len(con_choice)>0 and len(lib_choice)>0:\n",
    "        libandcon_cases.append(doc_ind)\n",
    "    if len(con_choice)==0: \n",
    "        con_choice=lib_choice\n",
    "        if len(lib_choice)>0:\n",
    "            libnotcon_cases.append(doc_ind)\n",
    "    if len(lib_choice)==0:\n",
    "        lib_choice=con_choice\n",
    "        if len(con_choice)>0:\n",
    "            connotlib_cases.append(doc_ind)\n",
    "    if len(con_choice)==0 and len(lib_choice)==0:\n",
    "        con_choice='asdfasdfasdfasdf'\n",
    "        lib_choice='asdfasdfasdfasdf'\n",
    "        fail_cases.append(doc_ind)\n",
    "    conservative_inputs.append(con_choice)\n",
    "    liberal_inputs.append(lib_choice)\n",
    "\n",
    "\n",
    "print('Len libandcon_cases: ' + str(len(libandcon_cases)))\n",
    "print('Len libnotcon_cases: ' + str(len(libnotcon_cases)))\n",
    "print('Len connotlib_cases: ' + str(len(connotlib_cases)))\n",
    "print('Len fail_cases: ' + str(len(fail_cases)))\n",
    "print(fail_cases)\n",
    "print('Sum: ' + str(len(libandcon_cases)+len(libnotcon_cases)+len(connotlib_cases)+len(fail_cases)))\n",
    "print(len(the_targets))\n",
    "print('Len conservative: ' + str(len(conservative_inputs)))\n",
    "print('Len liberal: ' + str(len(liberal_inputs)))\n",
    "con_data = pd.DataFrame(list(zip(the_targets, conservative_inputs)), columns =['targets', 'texts']) \n",
    "# lib_data = pd.DataFrame(list(zip(the_targets, liberal_inputs)), columns =['targets', 'texts']) \n",
    "# con_data = con_data.sample(frac=1).reset_index(drop=True)#shuffle\n",
    "# lib_data = lib_data.sample(frac=1).reset_index(drop=True)#shuffle\n",
    "print(con_data.head())\n",
    "# print(lib_data.head())\n",
    "#con_data\n",
    "# msk = np.random.rand(len(con_data)) < 0.9\n",
    "# con_train_data = con_data[msk].reset_index(drop=True)\n",
    "# con_test_data = con_data[~msk].reset_index(drop=True)\n",
    "con_train_data = con_data.copy()\n",
    "con_test_data = con_data.copy()\n",
    "print(con_test_data.head())\n",
    "print(np.shape(con_test_data))\n",
    "print(np.shape(con_train_data))\n",
    "con_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_cheat1/orig_data.tsv',sep='\\t',index=False,header=False)\n",
    "con_train_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_cheat1/train.tsv',sep='\\t',index=False,header=False)\n",
    "con_test_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_cheat1/dev.tsv',sep='\\t',index=False,header=False)\n",
    "print(the_dic[0].keys()) #Make sure these are the correct fields I'm using!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Create final-eval data - OLD\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import run_glue_gap\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_eval_original1.json'\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_eval_original2.json'\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_MT_test_no_overlap.json'\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_AETNA_test.json'\n",
    "filename='/home/ec2-user/SageMaker/Data/contexts_samples_MT_masked_test.json'\n",
    "\n",
    "\n",
    "#dict_keys(['query_name', 'contexts', 'article_title', 'conservative-marked-body', \n",
    "    #'conservative-sequence', 'liberal-marked-body', 'entity_relevance', 'article_body', 'liberal-sequence'])\n",
    "with open(filename, 'r') as f:\n",
    "            the_dic = json.loads(f.read())\n",
    "\n",
    "libandcon_cases=[]\n",
    "libnotcon_cases=[]\n",
    "atleastone_cases=[]\n",
    "fail_cases=[]\n",
    "#the_targets=[]\n",
    "the_targets=[docca['entity_relevance'] for docca in the_dic]\n",
    "conservative_inputs=[]\n",
    "liberal_inputs=[]\n",
    "for doc_ind, docca in enumerate(the_dic):\n",
    "    con_body=docca['conservative-marked-body'].replace('\\n','')\n",
    "    con_seq=docca['conservative-sequence'].replace('\\n','')\n",
    "    lib_body=docca['liberal-marked-body'].replace('\\n','')\n",
    "    lib_seq=docca['liberal-sequence'].replace('\\n','')\n",
    "    if len(lib_seq)>0 and len(lib_body)>0: #at least lib_seq and lib_body not empty\n",
    "        if len(lib_body.split())<len(lib_seq.split()):\n",
    "            liberal_inputs.append(lib_body)\n",
    "        else:\n",
    "            liberal_inputs.append(lib_seq)\n",
    "        if len(con_seq)>0 and len(con_body)>0: #con_seq and con_body also not empty\n",
    "            libandcon_cases.append(doc_ind)\n",
    "            if len(con_body.split())<len(con_seq.split()):\n",
    "                conservative_inputs.append(con_body)\n",
    "            else:\n",
    "                conservative_inputs.append(con_seq)\n",
    "        else: #lib not empty but con empty\n",
    "            libnotcon_cases.append(doc_ind)\n",
    "            conservative_inputs.append(liberal_inputs[-1])        \n",
    "        #the_targets.append(docca['entity_relevance'])\n",
    "    elif len(lib_seq)>0 or len(lib_body)>0 or len(con_seq)>0 or len(con_body)>0:\n",
    "        atleastone_cases.append(doc_ind)\n",
    "        if len(con_seq)>0:\n",
    "            conservative_inputs.append(con_seq)\n",
    "            liberal_inputs.append(con_seq)\n",
    "        elif len(con_body)>0:\n",
    "            conservative_inputs.append(con_body)\n",
    "            liberal_inputs.append(con_body)\n",
    "        elif len(lib_seq)>0:\n",
    "            conservative_inputs.append(lib_seq)\n",
    "            liberal_inputs.append(lib_seq)\n",
    "        elif len(lib_body)>0:\n",
    "            conservative_inputs.append(lib_body)\n",
    "            liberal_inputs.append(lib_body)\n",
    "        #the_targets.append(docca['entity_relevance'])\n",
    "    else:\n",
    "        #the_targets.append('0')\n",
    "        conservative_inputs.append('asdfasdfasdfasdf')\n",
    "        liberal_inputs.append('asdfasdfasdfasdf')\n",
    "        fail_cases.append(doc_ind)\n",
    "\n",
    "print(len(libandcon_cases))\n",
    "print(len(libnotcon_cases))\n",
    "print(len(atleastone_cases))\n",
    "print(fail_cases)\n",
    "print(len(the_targets))\n",
    "print(len(conservative_inputs))\n",
    "print(len(liberal_inputs))\n",
    "con_data = pd.DataFrame(list(zip(the_targets, conservative_inputs)), columns =['targets', 'texts']) \n",
    "lib_data = pd.DataFrame(list(zip(the_targets, liberal_inputs)), columns =['targets', 'texts']) \n",
    "#con_data = con_data.sample(frac=1).reset_index(drop=True)#shuffle\n",
    "#lib_data = lib_data.sample(frac=1).reset_index(drop=True)#shuffle\n",
    "print(con_data.head())\n",
    "print(lib_data.head())\n",
    "#con_data\n",
    "#msk = np.random.rand(len(con_data)) < 0.9\n",
    "con_train_data = con_data.copy()\n",
    "con_test_data = con_data.copy()\n",
    "print(con_test_data.head())\n",
    "print(np.shape(con_test_data))\n",
    "con_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_con_eval_masked/orig_data.tsv',sep='\\t',index=False,header=False)\n",
    "con_train_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_con_eval_masked/train.tsv',sep='\\t',index=False,header=False)\n",
    "con_test_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_con_eval_masked/dev.tsv',sep='\\t',index=False,header=False)\n",
    "#lib_data\n",
    "#msk = np.random.rand(len(lib_data)) < 0.9\n",
    "lib_train_data = lib_data.copy()\n",
    "lib_test_data = lib_data.copy()\n",
    "print(lib_test_data.head())\n",
    "print(np.shape(lib_test_data))\n",
    "lib_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_lib_eval_masked/orig_data.tsv',sep='\\t',index=False,header=False)\n",
    "lib_train_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_lib_eval_masked/train.tsv',sep='\\t',index=False,header=False)\n",
    "lib_test_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_lib_eval_masked/dev.tsv',sep='\\t',index=False,header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create final eval data - \n",
    "\n",
    "# For contexts_samples_MT_train_new:\n",
    "# #  <BEGIN-SPAN> Kyocera <END-SPAN>  is the tag\n",
    "# 'conservative-marked-body', \n",
    "# 'conservative-sequence',\n",
    "# 'liberal-marked-body', \n",
    "# 'liberal-sequence', \n",
    "# # Kyocera is the tag, (i.e. without <BEGIN-SPAN> and <END-SPAN> markers)\n",
    "# 'conservative-sequence-nospan', \n",
    "# 'liberal-sequence-nospan', \n",
    "# 'liberal-marked-body-nospan'\n",
    "# 'conservative-marked-body-nospan', \n",
    "# # <BEGIN-SPAN> ORGANIZATION1 <END-SPAN> is the tag\n",
    "# 'conservative-org-marked-body', \n",
    "# 'liberal-org-sequence', \n",
    "# 'liberal-org-marked-body', \n",
    "# 'conservative-org-sequence',\n",
    "# # ORGANIZATION1 is the tag, (i.e. without <BEGIN-SPAN> and <END-SPAN> markers)\n",
    "# 'conservative-org-marked-body-nospan'\n",
    "# 'conservative-org-sequence-nospan', \n",
    "# 'liberal-org-sequence-nospan', \n",
    "# 'liberal-org-marked-body-nospan', \n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import run_glue_gap\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_eval_original1.json'\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_eval_original2.json'\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_MT_test_no_overlap.json'\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_AETNA_test.json'\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_MT_masked_test.json'\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_MT_test_new.json'\n",
    "#filename='/home/projects/data/contexts_samples_MT_test_new.json'\n",
    "filename='/home/ec2-user/SageMaker/Data/contexts_samples_aetna_new.json'\n",
    "#filename='/home/ec2-user/SageMaker/Data/contexts_samples_Aetna_normalized.json'\n",
    "\n",
    "\n",
    "#dict_keys(['query_name', 'contexts', 'article_title', 'conservative-marked-body', \n",
    "    #'conservative-sequence', 'liberal-marked-body', 'entity_relevance', 'article_body', 'liberal-sequence'])\n",
    "with open(filename, 'r') as f:\n",
    "            the_dic = json.loads(f.read())\n",
    "\n",
    "libandcon_cases=[]\n",
    "libnotcon_cases=[]\n",
    "connotlib_cases=[]\n",
    "fail_cases=[]\n",
    "#the_targets=[]\n",
    "the_targets=[docca['entity_relevance'] for docca in the_dic]\n",
    "conservative_inputs=[]\n",
    "liberal_inputs=[]\n",
    "for doc_ind, docca in enumerate(the_dic):\n",
    "    # for NORMAL e.g. <BEGIN-SPAN> Kyocera <END-SPAN>  is the tag\n",
    "#     con_body=docca['conservative-marked-body'].replace('\\n','')\n",
    "#     con_seq=docca['conservative-sequence'].replace('\\n','')\n",
    "#     lib_body=docca['liberal-marked-body'].replace('\\n','')\n",
    "#     lib_seq=docca['liberal-sequence'].replace('\\n','')\n",
    "#     # for e.g. Kyocera is the tag, (i.e. without <BEGIN-SPAN> and <END-SPAN> markers)\n",
    "    con_body=docca['conservative-marked-body-nospan'].replace('\\n','')\n",
    "    con_seq=docca['conservative-sequence-nospan'].replace('\\n','')\n",
    "    lib_body=docca['liberal-marked-body-nospan'].replace('\\n','')\n",
    "    lib_seq=docca['liberal-sequence-nospan'].replace('\\n','')\n",
    "#     # for e.g. <BEGIN-SPAN> ORGANIZATION1 <END-SPAN> is the tag\n",
    "#     con_body=docca['conservative-org-marked-body'].replace('\\n','')\n",
    "#     con_seq=docca['conservative-org-sequence'].replace('\\n','')\n",
    "#     lib_body=docca['liberal-org-marked-body'].replace('\\n','')\n",
    "#     lib_seq=docca['liberal-org-sequence'].replace('\\n','')\n",
    "#     # for e.g. ORGANIZATION1 is the tag, (i.e. without <BEGIN-SPAN> and <END-SPAN> markers)\n",
    "#     con_body=docca['conservative-org-marked-body-nospan'].replace('\\n','')\n",
    "#     con_seq=docca['conservative-org-sequence-nospan'].replace('\\n','')\n",
    "#     lib_body=docca['liberal-org-marked-body-nospan'].replace('\\n','')\n",
    "#     lib_seq=docca['liberal-org-sequence-nospan'].replace('\\n','')\n",
    "    \n",
    "    lib_choice=''\n",
    "    con_choice=''\n",
    "    \n",
    "    if len(lib_seq)>0 and len(lib_body)>0: # if both lib_seq and lib_body not empty\n",
    "        if len(lib_body.split())<len(lib_seq.split()): #choose the shorter bw lib_body and lib_seq to append to lib\n",
    "            lib_choice=lib_body\n",
    "        else:\n",
    "            lib_choice=lib_seq\n",
    "    else: #if only one or neither are not empty\n",
    "        lib_choice=lib_seq if len(lib_seq)>0 else lib_body if len(lib_body)>0 else lib_choice            \n",
    "    if len(con_seq)>0 and len(con_body)>0: # if con_seq and con_body not empty\n",
    "        if len(con_body.split())<len(con_seq.split()): #choose the shorter bw con_body and con_seq to append to con\n",
    "            con_choice=con_body\n",
    "        else:\n",
    "            con_choice=con_seq\n",
    "    else: #if only one or neither are not empty\n",
    "        con_choice=con_seq if len(con_seq)>0 else con_body if len(con_body)>0 else con_choice  \n",
    "    if len(con_choice)>0 and len(lib_choice)>0:\n",
    "        libandcon_cases.append(doc_ind)\n",
    "    if len(con_choice)==0: \n",
    "        con_choice=lib_choice\n",
    "        if len(lib_choice)>0:\n",
    "            libnotcon_cases.append(doc_ind)\n",
    "    if len(lib_choice)==0:\n",
    "        lib_choice=con_choice\n",
    "        if len(con_choice)>0:\n",
    "            connotlib_cases.append(doc_ind)\n",
    "    if len(con_choice)==0 and len(lib_choice)==0:\n",
    "        con_choice='asdfasdfasdfasdf'\n",
    "        lib_choice='asdfasdfasdfasdf'\n",
    "        fail_cases.append(doc_ind)\n",
    "    conservative_inputs.append(con_choice)\n",
    "    liberal_inputs.append(lib_choice)\n",
    "\n",
    "\n",
    "print('Len libandcon_cases: ' + str(len(libandcon_cases)))\n",
    "print('Len libnotcon_cases: ' + str(len(libnotcon_cases)))\n",
    "print('Len connotlib_cases: ' + str(len(connotlib_cases)))\n",
    "print('Len fail_cases: ' + str(len(fail_cases)))\n",
    "print(fail_cases)\n",
    "print('Sum: ' + str(len(libandcon_cases)+len(libnotcon_cases)+len(connotlib_cases)+len(fail_cases)))\n",
    "print(len(the_targets))\n",
    "print('Len conservative: ' + str(len(conservative_inputs)))\n",
    "print('Len liberal: ' + str(len(liberal_inputs)))\n",
    "con_data = pd.DataFrame(list(zip(the_targets, conservative_inputs)), columns =['targets', 'texts']) \n",
    "lib_data = pd.DataFrame(list(zip(the_targets, liberal_inputs)), columns =['targets', 'texts']) \n",
    "#con_data = con_data.sample(frac=1).reset_index(drop=True)#shuffle\n",
    "#lib_data = lib_data.sample(frac=1).reset_index(drop=True)#shuffle\n",
    "print(con_data.head())\n",
    "print(lib_data.head())\n",
    "#con_data\n",
    "#msk = np.random.rand(len(con_data)) < 0.9\n",
    "con_train_data = con_data.copy()\n",
    "con_test_data = con_data.copy()\n",
    "print(con_test_data.head())\n",
    "print(np.shape(con_test_data))\n",
    "con_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_con_eval_aetna_new/orig_data.tsv',sep='\\t',index=False,header=False)\n",
    "con_train_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_con_eval_aetna_new/train.tsv',sep='\\t',index=False,header=False)\n",
    "con_test_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_con_eval_aetna_new/dev.tsv',sep='\\t',index=False,header=False)\n",
    "#lib_data\n",
    "#msk = np.random.rand(len(lib_data)) < 0.9\n",
    "lib_train_data = lib_data.copy()\n",
    "lib_test_data = lib_data.copy()\n",
    "print(lib_test_data.head())\n",
    "print(np.shape(lib_test_data))\n",
    "lib_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_lib_eval_aetna_new/orig_data.tsv',sep='\\t',index=False,header=False)\n",
    "lib_train_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_lib_eval_aetna_new/train.tsv',sep='\\t',index=False,header=False)\n",
    "lib_test_data.to_csv('/home/ec2-user/SageMaker/Data/entrel_data_lib_eval_aetna_new/dev.tsv',sep='\\t',index=False,header=False)\n",
    "print(the_dic[0].keys()) #Make sure these are the correct fields I'm using!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Glancy Prongay & Murray LLP Announces Investigation on Behalf of <BEGIN-SPAN> ternium <END-SPAN> S.A. Investors ( TX ). LOS ANGELES ( BUSINESS WIRE ) Glancy Prongay & Murray LLP ( “ GPM ” ) announces an investigation on behalf of <BEGIN-SPAN> ternium <END-SPAN> S.A. ( “ Tenaris ” or the “ Company ” ) ( NYSE. ternium TX ) investors concerning the Company and its officers’ possible violations of federal securities laws. ternium If you are a shareholder who suffered a loss, click here to participate. TX ) investors concerning the Company and its officers’ possible violations of federal securities laws ternium. If you are a shareholder who suffered a loss, click here to participate ternium. On November 27 , 2018 , Bloomberg published an article alleging that <BEGIN-SPAN> ternium <END-SPAN> ’ s Chairman Paolo Rocca , was indicted for his role in a graft scheme. ternium Specifically the article alleges that, “the judge charged Rocca after the Argentine billionaire testified that one of his company’s executives paid an undisclosed amount of cash to government officials in monthly installments from 2009 to 2012.” On this news, Ternium’s share price fell $1.42 per share or nearly 5% to close at $28.02 per share on November 27, 2018, thereby injuring investors. ternium Follow us for updates on Twitter. If you are a shareholder who suffered a loss, click here to participate ternium. On November 27, 2018, Bloomberg published an article alleging that Ternium’s Chairman Paolo Rocca, was indicted for his role in a graft scheme ternium. Specifically the article alleges that , “ the judge charged Rocca after the Argentine billionaire testified that one of his company ’ s executives paid an undisclosed amount of cash to government officials in monthly installments from 2009 to 2012. ” On this news , <BEGIN-SPAN> ternium <END-SPAN> ’ s share price fell $ 1.42 per share or nearly 5 % to close at $ 28.02 per share on November 27 , 2018 , thereby injuring investors. ternium Follow us for updates on Twitter. ternium twitter.com/GPM LLP . If you purchased Ternium, have information or would like to learn more about these claims, or have any questions concerning this announcement or your rights or interests with respect to these matters, please contact Lesley Portnoy, Esquire, of GPM, 1925 Century Park East, Suite 2100, Los Angeles, California 90067 at 310 201 9150, Toll Free at 888 773 9224, by email to shareholders@glancylaw.com , or visit our website at www.glancylaw.com . If you inquire by email please include your mailing address, telephone number and number of shares purchased. Specifically the article alleges that, “the judge charged Rocca after the Argentine billionaire testified that one of his company’s executives paid an undisclosed amount of cash to government officials in monthly installments from 2009 to 2012.” On this news, Ternium’s share price fell $1.42 per share or nearly 5% to close at $28.02 per share on November 27, 2018, thereby injuring investors ternium. Follow us for updates on Twitter ternium. twitter.com/GPM LLP . If you purchased <BEGIN-SPAN> ternium <END-SPAN> , have information'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_data.iloc[0]['texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['article_body', 'contexts', 'sequence', 'article_title', 'query_name', 'entity_relevance'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_dic[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Glancy Prongay & Murray LLP Announces Investigation on Behalf of <BEGIN-SPAN> ternium <END-SPAN> S.A. Investors ( TX ). LOS ANGELES ( BUSINESS WIRE ) Glancy Prongay & Murray LLP ( “ GPM ” ) announces an investigation on behalf of <BEGIN-SPAN> ternium <END-SPAN> S.A. ( “ Tenaris ” or the “ Company ” ) ( NYSE. ternium  TX ) investors concerning the Company and its officers’ possible violations of federal securities laws. ternium   If you are a shareholder who suffered a loss, click here to participate. TX ) investors concerning the Company and its officers’ possible violations of federal securities laws   ternium. If you are a shareholder who suffered a loss, click here to participate  ternium. On November 27 , 2018 , Bloomberg published an article alleging that <BEGIN-SPAN> ternium <END-SPAN> ’ s Chairman Paolo Rocca , was indicted for his role in a graft scheme. ternium  Specifically the article alleges that, “the judge charged Rocca after the Argentine billionaire testified that one of his company’s executives paid an undisclosed amount of cash to government officials in monthly installments from 2009 to 2012.” On this news, Ternium’s share price fell $1.42 per share or nearly 5% to close at $28.02 per share on November 27, 2018, thereby injuring investors. ternium   Follow us for updates on Twitter. If you are a shareholder who suffered a loss, click here to participate   ternium. On November 27, 2018, Bloomberg published an article alleging that Ternium’s Chairman Paolo Rocca, was indicted for his role in a graft scheme  ternium. Specifically the article alleges that , “ the judge charged Rocca after the Argentine billionaire testified that one of his company ’ s executives paid an undisclosed amount of cash to government officials in monthly installments from 2009 to 2012. ” On this news , <BEGIN-SPAN> ternium <END-SPAN> ’ s share price fell $ 1.42 per share or nearly 5 % to close at $ 28.02 per share on November 27 , 2018 , thereby injuring investors. ternium  Follow us for updates on Twitter. ternium   twitter.com/GPM LLP . If you purchased Ternium, have information or would like to learn more about these claims, or have any questions concerning this announcement or your rights or interests with respect to these matters, please contact Lesley Portnoy, Esquire, of GPM, 1925 Century Park East, Suite 2100, Los Angeles, California 90067 at 310 201 9150, Toll Free at 888 773 9224, by email to shareholders@glancylaw.com , or visit our website at www.glancylaw.com . If you inquire by email please include your mailing address, telephone number and number of shares purchased. Specifically the article alleges that, “the judge charged Rocca after the Argentine billionaire testified that one of his company’s executives paid an undisclosed amount of cash to government officials in monthly installments from 2009 to 2012.” On this news, Ternium’s share price fell $1.42 per share or nearly 5% to close at $28.02 per share on November 27, 2018, thereby injuring investors   ternium. Follow us for updates on Twitter  ternium. twitter.com/GPM LLP . If you purchased <BEGIN-SPAN> ternium <END-SPAN> , have information or would like to learn more about these claims , or have any questions concerning this announcement or your rights or interests with respect to these matters , please contact Lesley Portnoy , Esquire , of GPM , 1925 Century Park East , Suite 2100 , Los Angeles , California 90067 at 310 201 9150 , Toll Free at 888 773 9224 , by email to shareholders @ glancylaw.com , or visit our website at www.glancylaw.com . If you inquire by email please include your mailing address , telephone number and number of shares purchased. ternium  This press release may be considered Attorney Advertising in some jurisdictions under the applicable law and ethical rules. ternium   View source version on businesswire.com'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_dic[0]['sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LOS ANGELES--(BUSINESS WIRE)--       Glancy      Prongay & Murray LLP  (“GPM”) announces an investigation on      behalf of Ternium S.A.   (“Tenaris” or the “Company”) (NYSE:  TX )      investors concerning the Company and its officers’ possible violations      of federal securities laws.    \\n          If you are a shareholder who suffered a loss, click\\xa0 here \\xa0to      participate.    \\n          On\\xa0November 27, 2018,\\xa0Bloomberg\\xa0published an article alleging that      Ternium’s Chairman Paolo Rocca, was indicted for his role in a graft      scheme. Specifically the article alleges that, “the judge charged Rocca      after the Argentine billionaire testified that one of his company’s      executives paid an undisclosed amount of cash to government officials in      monthly installments from 2009 to 2012.” On this news, Ternium’s share      price fell $1.42 per share or nearly 5% to close at $28.02 per share on      November 27, 2018, thereby injuring investors.    \\n          Follow us for updates on Twitter:  twitter.com/GPM_LLP .    \\n          If you purchased Ternium, have information or would like to learn more      about these claims, or have any questions concerning this announcement      or your rights or interests with respect to these matters, please      contact Lesley Portnoy, Esquire, of GPM, 1925 Century Park East, Suite      2100, Los Angeles, California 90067 at 310-201-9150, Toll-Free at      888-773-9224, by email to  shareholders@glancylaw.com ,      or visit our website at  www.glancylaw.com .      If you inquire by email please include your mailing address, telephone      number and number of shares purchased.    \\n          This press release may be considered Attorney Advertising in some      jurisdictions under the applicable law and ethical rules.    \\n  \\nView source version on  businesswire.com :  https://www.businesswire.com/news/home/20181129005879/en/ \\n          Glancy Prongay & Murray LLP, Los Angeles/New YorkLesley      Portnoy, 310-201-9150 or 888-773-9224 shareholders@glancylaw.com  www.glancylaw.com     \\nSource: Glancy Prongay & Murray LLP'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_dic[0]['article_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Run one\n",
    "\n",
    "#contexts_samples_MT_train_new.json, epochs=13, bs=8, model_typea=2, small_or_big=0, lr=3.3e-5, warmup_steps= 53\n",
    "#MT_test_new: 0.78, aetna_new: 0.68\n",
    "#contexts_samples_MT_train_new.json, lr=2.27e--5, epochs=5, bs=8, warmup_steps= 86, model_typea=2, small_or_big=0, \n",
    "#MT_test_new: 0.86, aetna_new: 0.75\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import run_glue_gap\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "parser = run_glue_gap.argparse.ArgumentParser()\n",
    "args = run_glue_gap.get_args(parser)\n",
    "\n",
    "#datasetta=1 # 0: Gap, 1: entrel\n",
    "model_typea=2 # 0: BERT, 1: XLNET, 2: Roberta, 3: Distilbert, 4: XLM, 5:TransformerXL\n",
    "small_or_big=0 # 0: small, 1: big\n",
    "args.con_or_lib=0 # 0: con, 1: lib, 2: original\n",
    "batch_size=8\n",
    "\n",
    "args.num_train_epochs=5.0\n",
    "args.do_train=True\n",
    "args.do_eval=True\n",
    "\n",
    "#args.max_steps=3\n",
    "args.overwrite_output_dir=True\n",
    "args.overwrite_cache = True\n",
    "args.evaluate_during_training=False\n",
    "args.logging_steps =50\n",
    "args.save_steps=1000\n",
    "args.max_seq_length = 512\n",
    "args.seed=round(time.time())\n",
    "\n",
    "args.learning_rate =2.27e-5\n",
    "args.adam_epsilon=1e-8\n",
    "args.warmup_steps= 86  \n",
    "args.weight_decay= 0.0\n",
    "\n",
    "if small_or_big==0:\n",
    "    args.per_gpu_eval_batch_size=8\n",
    "    args.per_gpu_train_batch_size=8\n",
    "    if batch_size==8:\n",
    "        args.gradient_accumulation_steps=1\n",
    "    elif batch_size==16:\n",
    "        args.gradient_accumulation_steps=2\n",
    "    elif batch_size==32:\n",
    "        args.gradient_accumulation_steps=4\n",
    "elif small_or_big==1:\n",
    "    args.per_gpu_eval_batch_size=1\n",
    "    args.per_gpu_train_batch_size=1\n",
    "    if batch_size==8:\n",
    "        args.gradient_accumulation_steps=8\n",
    "    elif batch_size==16:\n",
    "        args.gradient_accumulation_steps=16\n",
    "    elif batch_size==32:\n",
    "        args.gradient_accumulation_steps=32\n",
    "\n",
    "\n",
    "\n",
    "if model_typea==0:\n",
    "    args.model_type='bert'\n",
    "    args.do_lower_case=True\n",
    "    if small_or_big==0:\n",
    "        args.model_name_or_path='bert-base-uncased'\n",
    "    else:\n",
    "        args.model_name_or_path='bert-large-uncased'\n",
    "elif model_typea==1:\n",
    "    args.model_type='xlnet'\n",
    "    args.do_lower_case=False\n",
    "    #args.model_name_or_path='xlnet-base-cased'\n",
    "    if small_or_big==0:\n",
    "        args.model_name_or_path='xlnet-base-cased'\n",
    "    else:\n",
    "        args.model_name_or_path='xlnet-large-cased'\n",
    "elif model_typea==2:\n",
    "    args.model_type='roberta'\n",
    "    args.do_lower_case=False\n",
    "    #args.model_name_or_path='roberta-base'\n",
    "    if small_or_big==0:\n",
    "        args.model_name_or_path='roberta-base'\n",
    "    else:\n",
    "        args.model_name_or_path='roberta-large'\n",
    "elif model_typea==3:\n",
    "    args.model_type='distilbert'\n",
    "    args.model_name_or_path='distilbert-base-uncased'\n",
    "    args.do_lower_case=True\n",
    "elif model_typea==4:\n",
    "    args.model_type='xlm'\n",
    "    args.model_name_or_path='xlm-mlm-en-2048'\n",
    "    args.do_lower_case=False\n",
    "\n",
    "# if datasetta==0:\n",
    "#     args.data_dir='/home/ec2-user/SageMaker/Data/gap_data/'\n",
    "#     args.output_dir='/tmp/gap/'\n",
    "#     args.task_name='gap'\n",
    "# elif datasetta==1:\n",
    "    \n",
    "if args.con_or_lib==0:\n",
    "    #args.data_dir='/home/ec2-user/SageMaker/Data/entrel_data_con'\n",
    "    args.data_dir='/home/ec2-user/SageMaker/Data/entrel_data_cheat1'\n",
    "elif args.con_or_lib==1:\n",
    "    args.data_dir='/home/ec2-user/SageMaker/Data/entrel_data_lib'\n",
    "elif args.con_or_lib==2:\n",
    "    args.data_dir='/home/ec2-user/SageMaker/Data/entrel_data'\n",
    "\n",
    "\n",
    "args.output_dir='/tmp/entrel/'\n",
    "shutil.rmtree(args.output_dir)\n",
    "args.task_name='entityrel'\n",
    "\n",
    "\n",
    "#args.per_gpu_eval_batch_size=2\n",
    "#args.per_gpu_train_batch_size=2\n",
    "#args.gradient_accumulation_steps=16\n",
    "\n",
    "results=run_glue_gap.run_main(parser,args)\n",
    "#shutil.rmtree(args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hypersearch... but don't use this. Use run_hypersearch.py instead\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import run_glue_gap\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "\n",
    "args_list=[]\n",
    "results_list=[]\n",
    "\n",
    "num_trials=1000\n",
    "\n",
    "for trial_num in range(num_trials):\n",
    "    \n",
    "    parser = run_glue_gap.argparse.ArgumentParser()\n",
    "    args = run_glue_gap.get_args(parser)\n",
    "    \n",
    "    #model_typea=0 # 0: BERT, 1: XLNET, 2: Roberta, 3: Distilbert\n",
    "    model_typea=run_glue_gap.gen_grid_val([0,1,2],'sel')\n",
    "    small_or_big=run_glue_gap.gen_grid_val([0,1],'sel') # 0: small, 1: big\n",
    "    args.con_or_lib=run_glue_gap.gen_grid_val([0,1],'sel') # 0: con, 1: lib\n",
    "    batch_size=run_glue_gap.gen_grid_val([8,16,32],'sel')\n",
    "    \n",
    "    args.num_train_epochs=run_glue_gap.gen_grid_val([5,8,11],'sel')\n",
    "    #args.num_train_epochs=run_glue_gap.gen_grid_val([1,2],'sel')\n",
    "    args.do_train=True\n",
    "    args.do_eval=True\n",
    "    \n",
    "    #args.max_steps=3\n",
    "    args.overwrite_output_dir=True\n",
    "    args.overwrite_cache = True\n",
    "    args.evaluate_during_training=False\n",
    "    args.logging_steps =50\n",
    "    args.save_steps=1000\n",
    "    args.max_seq_length = 512\n",
    "    \n",
    "    args.learning_rate =run_glue_gap.gen_grid_val([1e-6,5e-5],'exp')\n",
    "    args.adam_epsilon= 1e-8\n",
    "    args.warmup_steps= run_glue_gap.gen_grid_val([0,100],'lin_round')\n",
    "    args.weight_decay= 0.0\n",
    "    \n",
    "    if small_or_big==0:\n",
    "        args.per_gpu_eval_batch_size=8\n",
    "        args.per_gpu_train_batch_size=8\n",
    "        if batch_size==8:\n",
    "            args.gradient_accumulation_steps=1\n",
    "        elif batch_size==16:\n",
    "            args.gradient_accumulation_steps=2\n",
    "        elif batch_size==32:\n",
    "            args.gradient_accumulation_steps=4\n",
    "    elif small_or_big==1:\n",
    "        args.per_gpu_eval_batch_size=1\n",
    "        args.per_gpu_train_batch_size=1\n",
    "        if batch_size==8:\n",
    "            args.gradient_accumulation_steps=8\n",
    "        elif batch_size==16:\n",
    "            args.gradient_accumulation_steps=16\n",
    "        elif batch_size==32:\n",
    "            args.gradient_accumulation_steps=32\n",
    "    \n",
    "    if model_typea==0:\n",
    "        args.model_type='bert'\n",
    "        args.do_lower_case=True\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='bert-base-uncased'\n",
    "        else:\n",
    "            args.model_name_or_path='bert-large-uncased'\n",
    "    elif model_typea==1:\n",
    "        args.model_type='xlnet'\n",
    "        args.do_lower_case=False\n",
    "        #args.model_name_or_path='xlnet-base-cased'\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='xlnet-base-cased'\n",
    "        else:\n",
    "            args.model_name_or_path='xlnet-large-cased'\n",
    "    elif model_typea==2:\n",
    "        args.model_type='roberta'\n",
    "        args.do_lower_case=False\n",
    "        #args.model_name_or_path='roberta-base'\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='roberta-base'\n",
    "        else:\n",
    "            args.model_name_or_path='roberta-large'\n",
    "    elif model_typea==3:\n",
    "        args.model_type='distilbert'\n",
    "        args.model_name_or_path='distilbert-base-uncased'\n",
    "        args.do_lower_case=True\n",
    "    elif model_typea==4:\n",
    "        args.model_type='xlm'\n",
    "        args.model_name_or_path='xlm-mlm-en-2048'\n",
    "        args.do_lower_case=False\n",
    "    \n",
    "    if args.con_or_lib==0:\n",
    "        args.data_dir='/home/ec2-user/SageMaker/Data/entrel_data_con'\n",
    "    else:\n",
    "        args.data_dir='/home/ec2-user/SageMaker/Data/entrel_data_lib'\n",
    "    args.output_dir='/tmp/entrel/'\n",
    "    args.task_name='entityrel'\n",
    "    \n",
    "    results=run_glue_gap.run_main(parser,args)\n",
    "    \n",
    "    args_list.append(args)\n",
    "    results_list.append(results)\n",
    "    \n",
    "    outputta=(results_list,args_list)\n",
    "    pickle.dump(outputta, open(\"outputta.pickle\",\"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANALYZE\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import run_glue_gap\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "outputta = pickle.load( open( \"outputta.pickle\", \"rb\" ) )\n",
    "#print([outputta[0][inda]['f1_'] for inda in range(len(outputta[0]))])\n",
    "print([outputta[0][inda]['macrof1_'] for inda in range(len(outputta[0]))])\n",
    "#max_inds=[inda for inda in range(len(outputta[0])) if outputta[0][inda]['f1_']>0.87]\n",
    "max_inds=[inda for inda in range(len(outputta[0])) if outputta[0][inda]['macrof1_']>0.7]\n",
    "min_inds=[inda for inda in range(len(outputta[0])) if outputta[0][inda]['macrof1_']<0.6]\n",
    "for inda in max_inds:\n",
    "    #print(outputta[1][inda])\n",
    "    print('macrof1_: ' + str(outputta[0][inda]['macrof1_']))\n",
    "    #print('con_or_lib: ' + str(outputta[1][inda].con_or_lib))\n",
    "    #print('the_dataset: ' + str(outputta[1][inda].the_dataset))\n",
    "    print('learning_rate: ' + str(outputta[1][inda].learning_rate))\n",
    "    print('num_train_epochs: ' + str(outputta[1][inda].num_train_epochs))\n",
    "    print('per_gpu_train_batch_size: ' + str(outputta[1][inda].per_gpu_train_batch_size))\n",
    "    print('gradient_accumulation_steps: ' + str(outputta[1][inda].gradient_accumulation_steps))\n",
    "    print('warmup_steps: ' + str(outputta[1][inda].warmup_steps))\n",
    "    print('model_name_or_path: ' + str(outputta[1][inda].model_name_or_path))\n",
    "    print('...........')\n",
    "\n",
    "\n",
    "for inda in min_inds:\n",
    "    #print(outputta[1][inda])\n",
    "    print('macrof1_: ' + str(outputta[0][inda]['macrof1_']))\n",
    "    #print('con_or_lib: ' + str(outputta[1][inda].con_or_lib))\n",
    "    #print('the_dataset: ' + str(outputta[1][inda].the_dataset))\n",
    "    print('learning_rate: ' + str(outputta[1][inda].learning_rate))\n",
    "    print('num_train_epochs: ' + str(outputta[1][inda].num_train_epochs))\n",
    "    print('per_gpu_train_batch_size: ' + str(outputta[1][inda].per_gpu_train_batch_size))\n",
    "    print('gradient_accumulation_steps: ' + str(outputta[1][inda].gradient_accumulation_steps))\n",
    "    print('warmup_steps: ' + str(outputta[1][inda].warmup_steps))\n",
    "    print('model_name_or_path: ' + str(outputta[1][inda].model_name_or_path))\n",
    "    print('...........')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 5 separate datasets (k-fold leave 20% out)\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import run_glue_gap\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "\n",
    "con_data = pd.read_csv('/home/ec2-user/SageMaker/Data/entrel_data_con/orig_data.tsv',sep='\\t',names=['targets','texts'])\n",
    "print(con_data.head())\n",
    "print(np.shape(con_data))\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(con_data)\n",
    "print(kf)  \n",
    "X_train=[]\n",
    "X_test=[]\n",
    "bla_inda=0\n",
    "con_split=list(kf.split(con_data))\n",
    "for inda, train_indices in enumerate(con_split):\n",
    "    #bla1, bla2 = con_data[train_index], con_data[test_index]\n",
    "    X_train.append(con_data.iloc[train_indices[0]])\n",
    "    X_test.append(con_data.iloc[train_indices[1]])\n",
    "    print(np.shape(X_train[inda]))\n",
    "    print(np.shape(X_test[inda]))\n",
    "    X_train[inda].to_csv('/home/ec2-user/SageMaker/Data/entrel_data_con' + str(inda) + '/train.tsv',sep='\\t',index=False,header=False)\n",
    "    X_test[inda].to_csv('/home/ec2-user/SageMaker/Data/entrel_data_con' + str(inda) + '/dev.tsv',sep='\\t',index=False,header=False)\n",
    "    bla_inda+=1\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "#print(np.shape(X_train[0]))\n",
    "print(X_train[0].head())\n",
    "print(X_test[0].head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actually train on 5 separate datasets (k-fold leave 20% out)\n",
    "#DISTILBERT - best hyperparams\n",
    "\n",
    "for networka in range(5):\n",
    "    #\n",
    "    parser = run_glue_gap.argparse.ArgumentParser()\n",
    "    args = run_glue_gap.get_args(parser)\n",
    "    #\n",
    "    #datasetta=1 # 0: Gap, 1: entrel\n",
    "    model_typea=3 # 0: BERT, 1: XLNET, 2: Roberta, 3: Distilbert, 4: XLM, 5:TransformerXL\n",
    "    small_or_big=0 # 0: small, 1: big\n",
    "    args.con_or_lib=0 # 0: con, 1: lib, 2: original\n",
    "    batch_size=32\n",
    "    #\n",
    "    args.num_train_epochs=5.0\n",
    "    args.do_train=True\n",
    "    args.do_eval=True\n",
    "    #\n",
    "    #args.max_steps=3\n",
    "    args.overwrite_output_dir=True\n",
    "    args.overwrite_cache = True\n",
    "    args.evaluate_during_training=False\n",
    "    args.logging_steps =50\n",
    "    args.save_steps=1000\n",
    "    args.max_seq_length = 512\n",
    "    args.seed=round(time.time())\n",
    "    #\n",
    "    args.learning_rate =6.3e-5\n",
    "    args.adam_epsilon=1e-8\n",
    "    args.warmup_steps= 97  \n",
    "    args.weight_decay= 0.0\n",
    "    #\n",
    "    if small_or_big==0:\n",
    "        args.per_gpu_eval_batch_size=8\n",
    "        args.per_gpu_train_batch_size=8\n",
    "        if batch_size==8:\n",
    "            args.gradient_accumulation_steps=1\n",
    "        elif batch_size==16:\n",
    "            args.gradient_accumulation_steps=2\n",
    "        elif batch_size==32:\n",
    "            args.gradient_accumulation_steps=4\n",
    "    elif small_or_big==1:\n",
    "        args.per_gpu_eval_batch_size=1\n",
    "        args.per_gpu_train_batch_size=1\n",
    "        if batch_size==8:\n",
    "            args.gradient_accumulation_steps=8\n",
    "        elif batch_size==16:\n",
    "            args.gradient_accumulation_steps=16\n",
    "        elif batch_size==32:\n",
    "            args.gradient_accumulation_steps=32\n",
    "    #\n",
    "    if model_typea==0:\n",
    "        args.model_type='bert'\n",
    "        args.do_lower_case=True\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='bert-base-uncased'\n",
    "        else:\n",
    "            args.model_name_or_path='bert-large-uncased'\n",
    "    elif model_typea==1:\n",
    "        args.model_type='xlnet'\n",
    "        args.do_lower_case=False\n",
    "        #args.model_name_or_path='xlnet-base-cased'\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='xlnet-base-cased'\n",
    "        else:\n",
    "            args.model_name_or_path='xlnet-large-cased'\n",
    "    elif model_typea==2:\n",
    "        args.model_type='roberta'\n",
    "        args.do_lower_case=False\n",
    "        #args.model_name_or_path='roberta-base'\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='roberta-base'\n",
    "        else:\n",
    "            args.model_name_or_path='roberta-large'\n",
    "    elif model_typea==3:\n",
    "        args.model_type='distilbert'\n",
    "        args.model_name_or_path='distilbert-base-uncased'\n",
    "        args.do_lower_case=True\n",
    "    elif model_typea==4:\n",
    "        args.model_type='xlm'\n",
    "        args.model_name_or_path='xlm-mlm-en-2048'\n",
    "        args.do_lower_case=False\n",
    "    #\n",
    "    args.task_name='entityrel'\n",
    "    args.data_dir='/home/ec2-user/SageMaker/Data/entrel_data_con' + str(networka)\n",
    "    args.output_dir='/tmp/entrel/' + str(networka)\n",
    "    #\n",
    "    results=run_glue_gap.run_main(parser,args)\n",
    "    #shutil.rmtree(args.output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actually train on 5 separate datasets (k-fold leave 20% out)\n",
    "#XLNET - best hyperparams\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import run_glue_gap\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "\n",
    "for networka in range(5):\n",
    "    #\n",
    "    parser = run_glue_gap.argparse.ArgumentParser()\n",
    "    args = run_glue_gap.get_args(parser)\n",
    "    #\n",
    "    #datasetta=1 # 0: Gap, 1: entrel\n",
    "    model_typea=1 # 0: BERT, 1: XLNET, 2: Roberta, 3: Distilbert, 4: XLM, 5:TransformerXL\n",
    "    small_or_big=0 # 0: small, 1: big\n",
    "    args.con_or_lib=0 # 0: con, 1: lib, 2: original\n",
    "    batch_size=8 #16\n",
    "    #\n",
    "    args.num_train_epochs=5.0 #9.0\n",
    "    args.do_train=True\n",
    "    args.do_eval=True\n",
    "    #\n",
    "    #args.max_steps=3\n",
    "    args.overwrite_output_dir=True\n",
    "    args.overwrite_cache = True\n",
    "    args.evaluate_during_training=False\n",
    "    args.logging_steps =50\n",
    "    args.save_steps=1000\n",
    "    args.max_seq_length = 512\n",
    "    args.seed=round(time.time())\n",
    "    #\n",
    "    args.learning_rate = 2.1e-5 #7.6e-6\n",
    "    args.adam_epsilon=1e-8\n",
    "    args.warmup_steps= 90 # 97  \n",
    "    args.weight_decay= 0.0\n",
    "    #\n",
    "    if small_or_big==0:\n",
    "        args.per_gpu_eval_batch_size=8\n",
    "        args.per_gpu_train_batch_size=8\n",
    "        if batch_size==8:\n",
    "            args.gradient_accumulation_steps=1\n",
    "        elif batch_size==16:\n",
    "            args.gradient_accumulation_steps=2\n",
    "        elif batch_size==32:\n",
    "            args.gradient_accumulation_steps=4\n",
    "    elif small_or_big==1:\n",
    "        args.per_gpu_eval_batch_size=1\n",
    "        args.per_gpu_train_batch_size=1\n",
    "        if batch_size==8:\n",
    "            args.gradient_accumulation_steps=8\n",
    "        elif batch_size==16:\n",
    "            args.gradient_accumulation_steps=16\n",
    "        elif batch_size==32:\n",
    "            args.gradient_accumulation_steps=32\n",
    "    #\n",
    "    if model_typea==0:\n",
    "        args.model_type='bert'\n",
    "        args.do_lower_case=True\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='bert-base-uncased'\n",
    "        else:\n",
    "            args.model_name_or_path='bert-large-uncased'\n",
    "    elif model_typea==1:\n",
    "        args.model_type='xlnet'\n",
    "        args.do_lower_case=False\n",
    "        #args.model_name_or_path='xlnet-base-cased'\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='xlnet-base-cased'\n",
    "        else:\n",
    "            args.model_name_or_path='xlnet-large-cased'\n",
    "    elif model_typea==2:\n",
    "        args.model_type='roberta'\n",
    "        args.do_lower_case=False\n",
    "        #args.model_name_or_path='roberta-base'\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='roberta-base'\n",
    "        else:\n",
    "            args.model_name_or_path='roberta-large'\n",
    "    elif model_typea==3:\n",
    "        args.model_type='distilbert'\n",
    "        args.model_name_or_path='distilbert-base-uncased'\n",
    "        args.do_lower_case=True\n",
    "    elif model_typea==4:\n",
    "        args.model_type='xlm'\n",
    "        args.model_name_or_path='xlm-mlm-en-2048'\n",
    "        args.do_lower_case=False\n",
    "    #\n",
    "    args.task_name='entityrel'\n",
    "    args.data_dir='/home/ec2-user/SageMaker/Data/entrel_data_con' + str(networka)\n",
    "    args.output_dir='/tmp/entrel/' + str(networka)\n",
    "    #\n",
    "    results=run_glue_gap.run_main(parser,args)\n",
    "    #shutil.rmtree(args.output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actually train on 5 separate datasets (k-fold leave 20% out)\n",
    "#Roberta - best hyperparams\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import run_glue_gap\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "\n",
    "for networka in range(5):\n",
    "    #\n",
    "    parser = run_glue_gap.argparse.ArgumentParser()\n",
    "    args = run_glue_gap.get_args(parser)\n",
    "    #\n",
    "    #datasetta=1 # 0: Gap, 1: entrel\n",
    "    model_typea=2 # 0: BERT, 1: XLNET, 2: Roberta, 3: Distilbert, 4: XLM, 5:TransformerXL\n",
    "    small_or_big=0 # 0: small, 1: big\n",
    "    args.con_or_lib=0 # 0: con, 1: lib, 2: original\n",
    "    batch_size=16 #16\n",
    "    #\n",
    "    args.num_train_epochs=9.0\n",
    "    args.do_train=True\n",
    "    args.do_eval=True\n",
    "    #\n",
    "    #args.max_steps=3\n",
    "    args.overwrite_output_dir=True\n",
    "    args.overwrite_cache = True\n",
    "    args.evaluate_during_training=False\n",
    "    args.logging_steps =50\n",
    "    args.save_steps=1000\n",
    "    args.max_seq_length = 512\n",
    "    args.seed=round(time.time())\n",
    "    #\n",
    "    args.learning_rate = 2.4e-5 #7.6e-6\n",
    "    args.adam_epsilon=1e-8\n",
    "    args.warmup_steps= 120 # 97  \n",
    "    args.weight_decay= 0.0\n",
    "    #\n",
    "    if small_or_big==0:\n",
    "        args.per_gpu_eval_batch_size=8\n",
    "        args.per_gpu_train_batch_size=8\n",
    "        if batch_size==8:\n",
    "            args.gradient_accumulation_steps=1\n",
    "        elif batch_size==16:\n",
    "            args.gradient_accumulation_steps=2\n",
    "        elif batch_size==32:\n",
    "            args.gradient_accumulation_steps=4\n",
    "    elif small_or_big==1:\n",
    "        args.per_gpu_eval_batch_size=1\n",
    "        args.per_gpu_train_batch_size=1\n",
    "        if batch_size==8:\n",
    "            args.gradient_accumulation_steps=8\n",
    "        elif batch_size==16:\n",
    "            args.gradient_accumulation_steps=16\n",
    "        elif batch_size==32:\n",
    "            args.gradient_accumulation_steps=32\n",
    "    #\n",
    "    if model_typea==0:\n",
    "        args.model_type='bert'\n",
    "        args.do_lower_case=True\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='bert-base-uncased'\n",
    "        else:\n",
    "            args.model_name_or_path='bert-large-uncased'\n",
    "    elif model_typea==1:\n",
    "        args.model_type='xlnet'\n",
    "        args.do_lower_case=False\n",
    "        #args.model_name_or_path='xlnet-base-cased'\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='xlnet-base-cased'\n",
    "        else:\n",
    "            args.model_name_or_path='xlnet-large-cased'\n",
    "    elif model_typea==2:\n",
    "        args.model_type='roberta'\n",
    "        args.do_lower_case=False\n",
    "        #args.model_name_or_path='roberta-base'\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='roberta-base'\n",
    "        else:\n",
    "            args.model_name_or_path='roberta-large'\n",
    "    elif model_typea==3:\n",
    "        args.model_type='distilbert'\n",
    "        args.model_name_or_path='distilbert-base-uncased'\n",
    "        args.do_lower_case=True\n",
    "    elif model_typea==4:\n",
    "        args.model_type='xlm'\n",
    "        args.model_name_or_path='xlm-mlm-en-2048'\n",
    "        args.do_lower_case=False\n",
    "    #\n",
    "    args.task_name='entityrel'\n",
    "    args.data_dir='/home/ec2-user/SageMaker/Data/entrel_data_con' + str(networka)\n",
    "    args.output_dir='/tmp/entrel/' + str(networka)\n",
    "    #\n",
    "    results=run_glue_gap.run_main(parser,args)\n",
    "    #shutil.rmtree(args.output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate training set on 5 networks:\n",
    "# DISTILBERT\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import run_glue_gap\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,TensorDataset)\n",
    "import tqdm\n",
    "from transformers import (WEIGHTS_NAME, BertConfig,\n",
    "                                  BertForSequenceClassification, BertTokenizer,\n",
    "                                  RobertaConfig,\n",
    "                                  RobertaForSequenceClassification,\n",
    "                                  RobertaTokenizer,\n",
    "                                  XLMConfig, XLMForSequenceClassification,\n",
    "                                  XLMTokenizer, XLNetConfig,\n",
    "                                  XLNetForSequenceClassification,\n",
    "                                  XLNetTokenizer,\n",
    "                                  DistilBertConfig,\n",
    "                                  DistilBertForSequenceClassification,\n",
    "                                  DistilBertTokenizer)\n",
    "from utils_glue_gap import (compute_metrics,\n",
    "                        output_modes, processors, simple_accuracy)\n",
    "#\n",
    "all_preds=[]\n",
    "all_finalpreds=[]\n",
    "all_actuals=[]\n",
    "all_accs=[]\n",
    "#networka=0\n",
    "#\n",
    "for networka in range(5):\n",
    "    parser = run_glue_gap.argparse.ArgumentParser()\n",
    "    args = run_glue_gap.get_args(parser)\n",
    "    #\n",
    "    model_typea=3 # 0: BERT, 1: XLNET, 2: Roberta, 3: Distilbert, 4: XLM, 5:TransformerXL\n",
    "    small_or_big=0 # 0: small, 1: big\n",
    "    args.con_or_lib=0 # 0: con, 1: lib, 2: original\n",
    "    batch_size=32\n",
    "    #\n",
    "    args.num_train_epochs=5.0\n",
    "    args.do_train=True\n",
    "    args.do_eval=True\n",
    "    #\n",
    "    #args.max_steps=3\n",
    "    args.overwrite_output_dir=True\n",
    "    args.overwrite_cache = True\n",
    "    args.evaluate_during_training=False\n",
    "    args.logging_steps =50\n",
    "    args.save_steps=1000\n",
    "    args.max_seq_length = 512\n",
    "    args.seed=round(time.time())\n",
    "    #\n",
    "    args.learning_rate =6.3e-5\n",
    "    args.adam_epsilon=1e-8\n",
    "    args.warmup_steps= 97  \n",
    "    args.weight_decay= 0.0\n",
    "    #\n",
    "    if small_or_big==0:\n",
    "        args.per_gpu_eval_batch_size=8\n",
    "        args.per_gpu_train_batch_size=8\n",
    "        if batch_size==8:\n",
    "            args.gradient_accumulation_steps=1\n",
    "        elif batch_size==16:\n",
    "            args.gradient_accumulation_steps=2\n",
    "        elif batch_size==32:\n",
    "            args.gradient_accumulation_steps=4\n",
    "    elif small_or_big==1:\n",
    "        args.per_gpu_eval_batch_size=1\n",
    "        args.per_gpu_train_batch_size=1\n",
    "        if batch_size==8:\n",
    "            args.gradient_accumulation_steps=8\n",
    "        elif batch_size==16:\n",
    "            args.gradient_accumulation_steps=16\n",
    "        elif batch_size==32:\n",
    "            args.gradient_accumulation_steps=32\n",
    "    #\n",
    "    #\n",
    "    if model_typea==0:\n",
    "        args.model_type='bert'\n",
    "        args.do_lower_case=True\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='bert-base-uncased'\n",
    "        else:\n",
    "            args.model_name_or_path='bert-large-uncased'\n",
    "    elif model_typea==1:\n",
    "        args.model_type='xlnet'\n",
    "        args.do_lower_case=False\n",
    "        #args.model_name_or_path='xlnet-base-cased'\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='xlnet-base-cased'\n",
    "        else:\n",
    "            args.model_name_or_path='xlnet-large-cased'\n",
    "    elif model_typea==2:\n",
    "        args.model_type='roberta'\n",
    "        args.do_lower_case=False\n",
    "        #args.model_name_or_path='roberta-base'\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='roberta-base'\n",
    "        else:\n",
    "            args.model_name_or_path='roberta-large'\n",
    "    elif model_typea==3:\n",
    "        args.model_type='distilbert'\n",
    "        args.model_name_or_path='distilbert-base-uncased'\n",
    "        args.do_lower_case=True\n",
    "    elif model_typea==4:\n",
    "        args.model_type='xlm'\n",
    "        args.model_name_or_path='xlm-mlm-en-2048'\n",
    "        args.do_lower_case=False\n",
    "    #\n",
    "    #\n",
    "    args.task_name='entityrel'\n",
    "    args.data_dir='/home/ec2-user/SageMaker/Data/entrel_data_con' + str(networka)\n",
    "    args.output_dir='/tmp/entrel/' + str(networka)\n",
    "    #\n",
    "    results = {}\n",
    "    MODEL_CLASSES = {\n",
    "        'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "        'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
    "        'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
    "        'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
    "        'distilbert': (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer)\n",
    "    }\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        args.n_gpu = torch.cuda.device_count()\n",
    "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "        args.n_gpu = 1\n",
    "    #\n",
    "    args.device = device\n",
    "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "    tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "    checkpoints = [args.output_dir]\n",
    "    checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True)))\n",
    "    args.output_mode = output_modes[args.task_name]\n",
    "    checkpoint=checkpoints[-1]\n",
    "    global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
    "    prefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else \"\"\n",
    "    model = model_class.from_pretrained(checkpoint)\n",
    "    model.to(args.device)\n",
    "    #result = run_glue_gap.evaluate(args, model, tokenizer, prefix=prefix)\n",
    "    eval_task=args.task_name\n",
    "    eval_dataset = run_glue_gap.load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    #for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    for batch in eval_dataloader:\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels':         batch[3]}\n",
    "            if args.model_type != 'distilbert':\n",
    "                inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None  # XLM, DistilBERT and RoBERTa don't use segment_ids\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "    #\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    final_preds = np.argmax(preds, axis=1)\n",
    "    result = simple_accuracy(final_preds, out_label_ids)\n",
    "    nupreds=[list(map(float,list(preds[:,0]))),list(map(float, list(preds[:,1])))]\n",
    "    all_preds.append(nupreds)\n",
    "    all_finalpreds.append(list(map(float,list(final_preds))))\n",
    "    all_actuals.append(list(map(float,list(out_label_ids))))\n",
    "    all_accs.append(result)\n",
    "\n",
    "\n",
    "the_dic={'all_outputs':all_preds,'all_preds':all_finalpreds,'all_actuals':all_actuals,'all_accs':all_accs}\n",
    "filename='/home/ec2-user/SageMaker/Data/distilbert_traindata_masked_outputs.json'\n",
    "with open(filename, 'w') as outfile:\n",
    "    json.dump(the_dic, outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate training set on 5 networks:\n",
    "# XLNET / Roberta\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import run_glue_gap\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,TensorDataset)\n",
    "import tqdm\n",
    "from transformers import (WEIGHTS_NAME, BertConfig,\n",
    "                                  BertForSequenceClassification, BertTokenizer,\n",
    "                                  RobertaConfig,\n",
    "                                  RobertaForSequenceClassification,\n",
    "                                  RobertaTokenizer,\n",
    "                                  XLMConfig, XLMForSequenceClassification,\n",
    "                                  XLMTokenizer, XLNetConfig,\n",
    "                                  XLNetForSequenceClassification,\n",
    "                                  XLNetTokenizer,\n",
    "                                  DistilBertConfig,\n",
    "                                  DistilBertForSequenceClassification,\n",
    "                                  DistilBertTokenizer)\n",
    "from utils_glue_gap import (compute_metrics,\n",
    "                        output_modes, processors, simple_accuracy)\n",
    "#\n",
    "all_preds=[]\n",
    "all_finalpreds=[]\n",
    "all_actuals=[]\n",
    "all_accs=[]\n",
    "#networka=0\n",
    "#\n",
    "for networka in range(5):\n",
    "    parser = run_glue_gap.argparse.ArgumentParser()\n",
    "    args = run_glue_gap.get_args(parser)\n",
    "    #\n",
    "    model_typea=2 # 0: BERT, 1: XLNET, 2: Roberta, 3: Distilbert, 4: XLM, 5:TransformerXL\n",
    "    small_or_big=0 # 0: small, 1: big\n",
    "    args.con_or_lib=0 # 0: con, 1: lib, 2: original\n",
    "    batch_size=16\n",
    "    #\n",
    "    args.num_train_epochs=9\n",
    "    args.do_train=True\n",
    "    args.do_eval=True\n",
    "    #\n",
    "    #args.max_steps=3\n",
    "    args.overwrite_output_dir=True\n",
    "    args.overwrite_cache = True\n",
    "    args.evaluate_during_training=False\n",
    "    args.logging_steps =50\n",
    "    args.save_steps=1000\n",
    "    args.max_seq_length = 512\n",
    "    args.seed=round(time.time())\n",
    "    #\n",
    "    args.learning_rate =7.5e-6\n",
    "    args.adam_epsilon=1e-8\n",
    "    args.warmup_steps= 97  \n",
    "    args.weight_decay= 0.0\n",
    "    #\n",
    "    if small_or_big==0:\n",
    "        args.per_gpu_eval_batch_size=8\n",
    "        args.per_gpu_train_batch_size=8\n",
    "        if batch_size==8:\n",
    "            args.gradient_accumulation_steps=1\n",
    "        elif batch_size==16:\n",
    "            args.gradient_accumulation_steps=2\n",
    "        elif batch_size==32:\n",
    "            args.gradient_accumulation_steps=4\n",
    "    elif small_or_big==1:\n",
    "        args.per_gpu_eval_batch_size=1\n",
    "        args.per_gpu_train_batch_size=1\n",
    "        if batch_size==8:\n",
    "            args.gradient_accumulation_steps=8\n",
    "        elif batch_size==16:\n",
    "            args.gradient_accumulation_steps=16\n",
    "        elif batch_size==32:\n",
    "            args.gradient_accumulation_steps=32\n",
    "    #\n",
    "    #\n",
    "    if model_typea==0:\n",
    "        args.model_type='bert'\n",
    "        args.do_lower_case=True\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='bert-base-uncased'\n",
    "        else:\n",
    "            args.model_name_or_path='bert-large-uncased'\n",
    "    elif model_typea==1:\n",
    "        args.model_type='xlnet'\n",
    "        args.do_lower_case=False\n",
    "        #args.model_name_or_path='xlnet-base-cased'\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='xlnet-base-cased'\n",
    "        else:\n",
    "            args.model_name_or_path='xlnet-large-cased'\n",
    "    elif model_typea==2:\n",
    "        args.model_type='roberta'\n",
    "        args.do_lower_case=False\n",
    "        #args.model_name_or_path='roberta-base'\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='roberta-base'\n",
    "        else:\n",
    "            args.model_name_or_path='roberta-large'\n",
    "    elif model_typea==3:\n",
    "        args.model_type='distilbert'\n",
    "        args.model_name_or_path='distilbert-base-uncased'\n",
    "        args.do_lower_case=True\n",
    "    elif model_typea==4:\n",
    "        args.model_type='xlm'\n",
    "        args.model_name_or_path='xlm-mlm-en-2048'\n",
    "        args.do_lower_case=False\n",
    "    #\n",
    "    #\n",
    "    args.task_name='entityrel'\n",
    "    args.data_dir='/home/ec2-user/SageMaker/Data/entrel_data_con' + str(networka)\n",
    "    args.output_dir='/tmp/entrel/' + str(networka)\n",
    "    #\n",
    "    results = {}\n",
    "    MODEL_CLASSES = {\n",
    "        'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "        'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
    "        'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
    "        'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
    "        'distilbert': (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer)\n",
    "    }\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        args.n_gpu = torch.cuda.device_count()\n",
    "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "        args.n_gpu = 1\n",
    "    #\n",
    "    args.device = device\n",
    "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "    tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "    checkpoints = [args.output_dir]\n",
    "    checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True)))\n",
    "    args.output_mode = output_modes[args.task_name]\n",
    "    checkpoint=checkpoints[-1]\n",
    "    global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
    "    prefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else \"\"\n",
    "    model = model_class.from_pretrained(checkpoint)\n",
    "    model.to(args.device)\n",
    "    #result = run_glue_gap.evaluate(args, model, tokenizer, prefix=prefix)\n",
    "    eval_task=args.task_name\n",
    "    eval_dataset = run_glue_gap.load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    #for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    for batch in eval_dataloader:\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels':         batch[3]}\n",
    "            if args.model_type != 'distilbert':\n",
    "                inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None  # XLM, DistilBERT and RoBERTa don't use segment_ids\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "    #\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    final_preds = np.argmax(preds, axis=1)\n",
    "    result = simple_accuracy(final_preds, out_label_ids)\n",
    "    nupreds=[list(map(float,list(preds[:,0]))),list(map(float, list(preds[:,1])))]\n",
    "    all_preds.append(nupreds)\n",
    "    all_finalpreds.append(list(map(float,list(final_preds))))\n",
    "    all_actuals.append(list(map(float,list(out_label_ids))))\n",
    "    all_accs.append(result)\n",
    "\n",
    "\n",
    "the_dic={'all_outputs':all_preds,'all_preds':all_finalpreds,'all_actuals':all_actuals,'all_accs':all_accs}\n",
    "filename='/home/ec2-user/SageMaker/Data/roberta_traindata_outputs_new.json'\n",
    "with open(filename, 'w') as outfile:\n",
    "    json.dump(the_dic, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final test set on 5 networks:\n",
    "# DISTILBERT\n",
    "\n",
    "\n",
    "#the five networks must be moved back to /tmp/entrel/ for this to work (0, 1, 2, 3, 4)\n",
    "#e.g. mv /home/ec2-user/SageMaker/Data/entrel_five/  /tmp/entrel/\n",
    "#remember to move them back before shutting down!\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import run_glue_gap\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,TensorDataset)\n",
    "import tqdm\n",
    "from transformers import (WEIGHTS_NAME, BertConfig,\n",
    "                                  BertForSequenceClassification, BertTokenizer,\n",
    "                                  RobertaConfig,\n",
    "                                  RobertaForSequenceClassification,\n",
    "                                  RobertaTokenizer,\n",
    "                                  XLMConfig, XLMForSequenceClassification,\n",
    "                                  XLMTokenizer, XLNetConfig,\n",
    "                                  XLNetForSequenceClassification,\n",
    "                                  XLNetTokenizer,\n",
    "                                  DistilBertConfig,\n",
    "                                  DistilBertForSequenceClassification,\n",
    "                                  DistilBertTokenizer)\n",
    "from utils_glue_gap import (compute_metrics,\n",
    "                        output_modes, processors, simple_accuracy)\n",
    "#\n",
    "all_preds=[]\n",
    "all_finalpreds=[]\n",
    "all_actuals=[]\n",
    "all_accs=[]\n",
    "all_times=[]\n",
    "#networka=0\n",
    "#\n",
    "for networka in range(5):\n",
    "    parser = run_glue_gap.argparse.ArgumentParser()\n",
    "    args = run_glue_gap.get_args(parser)\n",
    "    #\n",
    "    model_typea=3 # 0: BERT, 1: XLNET, 2: Roberta, 3: Distilbert, 4: XLM, 5:TransformerXL\n",
    "    small_or_big=0 # 0: small, 1: big\n",
    "    args.con_or_lib=0 # 0: con, 1: lib, 2: original\n",
    "    batch_size=32\n",
    "    #\n",
    "    args.num_train_epochs=5.0\n",
    "    args.do_train=True\n",
    "    args.do_eval=True\n",
    "    #\n",
    "    #args.max_steps=3\n",
    "    args.overwrite_output_dir=True\n",
    "    args.overwrite_cache = True\n",
    "    args.evaluate_during_training=False\n",
    "    args.logging_steps =50\n",
    "    args.save_steps=1000\n",
    "    args.max_seq_length = 512\n",
    "    args.seed=round(time.time())\n",
    "    #\n",
    "    args.learning_rate =6.3e-5\n",
    "    args.adam_epsilon=1e-8\n",
    "    args.warmup_steps= 97  \n",
    "    args.weight_decay= 0.0\n",
    "    #\n",
    "    if small_or_big==0:\n",
    "        args.per_gpu_eval_batch_size=8\n",
    "        args.per_gpu_train_batch_size=8\n",
    "        if batch_size==8:\n",
    "            args.gradient_accumulation_steps=1\n",
    "        elif batch_size==16:\n",
    "            args.gradient_accumulation_steps=2\n",
    "        elif batch_size==32:\n",
    "            args.gradient_accumulation_steps=4\n",
    "    elif small_or_big==1:\n",
    "        args.per_gpu_eval_batch_size=1\n",
    "        args.per_gpu_train_batch_size=1\n",
    "        if batch_size==8:\n",
    "            args.gradient_accumulation_steps=8\n",
    "        elif batch_size==16:\n",
    "            args.gradient_accumulation_steps=16\n",
    "        elif batch_size==32:\n",
    "            args.gradient_accumulation_steps=32\n",
    "    #\n",
    "    #\n",
    "    if model_typea==0:\n",
    "        args.model_type='bert'\n",
    "        args.do_lower_case=True\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='bert-base-uncased'\n",
    "        else:\n",
    "            args.model_name_or_path='bert-large-uncased'\n",
    "    elif model_typea==1:\n",
    "        args.model_type='xlnet'\n",
    "        args.do_lower_case=False\n",
    "        #args.model_name_or_path='xlnet-base-cased'\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='xlnet-base-cased'\n",
    "        else:\n",
    "            args.model_name_or_path='xlnet-large-cased'\n",
    "    elif model_typea==2:\n",
    "        args.model_type='roberta'\n",
    "        args.do_lower_case=False\n",
    "        #args.model_name_or_path='roberta-base'\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='roberta-base'\n",
    "        else:\n",
    "            args.model_name_or_path='roberta-large'\n",
    "    elif model_typea==3:\n",
    "        args.model_type='distilbert'\n",
    "        args.model_name_or_path='distilbert-base-uncased'\n",
    "        args.do_lower_case=True\n",
    "    elif model_typea==4:\n",
    "        args.model_type='xlm'\n",
    "        args.model_name_or_path='xlm-mlm-en-2048'\n",
    "        args.do_lower_case=False\n",
    "    #\n",
    "    #\n",
    "    args.task_name='entityrel'\n",
    "    args.data_dir='/home/ec2-user/SageMaker/Data/entrel_data_con_eval_masked_AETNA'\n",
    "    #args.data_dir='/home/projects/data/entrel_data_con_eval2'\n",
    "    args.output_dir='/tmp/entrel/' + str(networka)\n",
    "    #\n",
    "    results = {}\n",
    "    MODEL_CLASSES = {\n",
    "        'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "        'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
    "        'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
    "        'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
    "        'distilbert': (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer)\n",
    "    }\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        args.n_gpu = torch.cuda.device_count()\n",
    "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "        args.n_gpu = 1\n",
    "    #\n",
    "    args.device = device\n",
    "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "    tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "    checkpoints = [args.output_dir]\n",
    "    checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True)))\n",
    "    args.output_mode = output_modes[args.task_name]\n",
    "    checkpoint=checkpoints[-1]\n",
    "    global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
    "    prefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else \"\"\n",
    "    model = model_class.from_pretrained(checkpoint)\n",
    "    model.to(args.device)\n",
    "    #result = run_glue_gap.evaluate(args, model, tokenizer, prefix=prefix)\n",
    "    eval_task=args.task_name\n",
    "    eval_dataset = run_glue_gap.load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    start_time=time.time()\n",
    "    #for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    for batch in eval_dataloader:\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels':         batch[3]}\n",
    "            if args.model_type != 'distilbert':\n",
    "                inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None  # XLM, DistilBERT and RoBERTa don't use segment_ids\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "    #\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    final_preds = np.argmax(preds, axis=1)\n",
    "    result = simple_accuracy(final_preds, out_label_ids)\n",
    "    nupreds=[list(map(float,list(preds[:,0]))),list(map(float, list(preds[:,1])))]\n",
    "    all_preds.append(nupreds)\n",
    "    all_finalpreds.append(list(map(float,list(final_preds))))\n",
    "    all_actuals.append(list(map(float,list(out_label_ids))))\n",
    "    all_accs.append(result)\n",
    "    all_times.append(time.time()-start_time)\n",
    "\n",
    "\n",
    "the_dic={'all_outputs':all_preds,'all_preds':all_finalpreds,'all_actuals':all_actuals,'all_accs':all_accs,'all_times':all_times}\n",
    "filename='/home/ec2-user/SageMaker/Data/distilbert_testdata_masked_AETNA_outputs.json'\n",
    "with open(filename, 'w') as outfile:\n",
    "    json.dump(the_dic, outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final test set on 5 networks:\n",
    "# XLNET / Roberta\n",
    "\n",
    "#the five networks must be moved back to /tmp/entrel/ for this to work (0, 1, 2, 3, 4)\n",
    "#e.g. mv /home/ec2-user/SageMaker/Data/entrel_five/  /tmp/entrel/\n",
    "#remember to move them back before shutting down!\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import run_glue_gap\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,TensorDataset)\n",
    "import tqdm\n",
    "from transformers import (WEIGHTS_NAME, BertConfig,\n",
    "                                  BertForSequenceClassification, BertTokenizer,\n",
    "                                  RobertaConfig,\n",
    "                                  RobertaForSequenceClassification,\n",
    "                                  RobertaTokenizer,\n",
    "                                  XLMConfig, XLMForSequenceClassification,\n",
    "                                  XLMTokenizer, XLNetConfig,\n",
    "                                  XLNetForSequenceClassification,\n",
    "                                  XLNetTokenizer,\n",
    "                                  DistilBertConfig,\n",
    "                                  DistilBertForSequenceClassification,\n",
    "                                  DistilBertTokenizer)\n",
    "from utils_glue_gap import (compute_metrics,\n",
    "                        output_modes, processors, simple_accuracy)\n",
    "#\n",
    "all_preds=[]\n",
    "all_finalpreds=[]\n",
    "all_actuals=[]\n",
    "all_accs=[]\n",
    "all_times=[]\n",
    "#networka=0\n",
    "#\n",
    "for networka in range(5):\n",
    "    parser = run_glue_gap.argparse.ArgumentParser()\n",
    "    args = run_glue_gap.get_args(parser)\n",
    "    #\n",
    "    model_typea=2 # 0: BERT, 1: XLNET, 2: Roberta, 3: Distilbert, 4: XLM, 5:TransformerXL\n",
    "    small_or_big=0 # 0: small, 1: big\n",
    "    args.con_or_lib=0 # 0: con, 1: lib, 2: original\n",
    "    batch_size=16\n",
    "    #\n",
    "    args.num_train_epochs=9\n",
    "    args.do_train=True\n",
    "    args.do_eval=True\n",
    "    #\n",
    "    #args.max_steps=3\n",
    "    args.overwrite_output_dir=True\n",
    "    args.overwrite_cache = True\n",
    "    args.evaluate_during_training=False\n",
    "    args.logging_steps =50\n",
    "    args.save_steps=1000\n",
    "    args.max_seq_length = 512\n",
    "    args.seed=round(time.time())\n",
    "    #\n",
    "    args.learning_rate =7.5e-6\n",
    "    args.adam_epsilon=1e-8\n",
    "    args.warmup_steps= 97  \n",
    "    args.weight_decay= 0.0\n",
    "    #\n",
    "    if small_or_big==0:\n",
    "        args.per_gpu_eval_batch_size=8\n",
    "        args.per_gpu_train_batch_size=8\n",
    "        if batch_size==8:\n",
    "            args.gradient_accumulation_steps=1\n",
    "        elif batch_size==16:\n",
    "            args.gradient_accumulation_steps=2\n",
    "        elif batch_size==32:\n",
    "            args.gradient_accumulation_steps=4\n",
    "    elif small_or_big==1:\n",
    "        args.per_gpu_eval_batch_size=1\n",
    "        args.per_gpu_train_batch_size=1\n",
    "        if batch_size==8:\n",
    "            args.gradient_accumulation_steps=8\n",
    "        elif batch_size==16:\n",
    "            args.gradient_accumulation_steps=16\n",
    "        elif batch_size==32:\n",
    "            args.gradient_accumulation_steps=32\n",
    "    #\n",
    "    #\n",
    "    if model_typea==0:\n",
    "        args.model_type='bert'\n",
    "        args.do_lower_case=True\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='bert-base-uncased'\n",
    "        else:\n",
    "            args.model_name_or_path='bert-large-uncased'\n",
    "    elif model_typea==1:\n",
    "        args.model_type='xlnet'\n",
    "        args.do_lower_case=False\n",
    "        #args.model_name_or_path='xlnet-base-cased'\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='xlnet-base-cased'\n",
    "        else:\n",
    "            args.model_name_or_path='xlnet-large-cased'\n",
    "    elif model_typea==2:\n",
    "        args.model_type='roberta'\n",
    "        args.do_lower_case=False\n",
    "        #args.model_name_or_path='roberta-base'\n",
    "        if small_or_big==0:\n",
    "            args.model_name_or_path='roberta-base'\n",
    "        else:\n",
    "            args.model_name_or_path='roberta-large'\n",
    "    elif model_typea==3:\n",
    "        args.model_type='distilbert'\n",
    "        args.model_name_or_path='distilbert-base-uncased'\n",
    "        args.do_lower_case=True\n",
    "    elif model_typea==4:\n",
    "        args.model_type='xlm'\n",
    "        args.model_name_or_path='xlm-mlm-en-2048'\n",
    "        args.do_lower_case=False\n",
    "    #\n",
    "    #\n",
    "    args.task_name='entityrel'\n",
    "    args.data_dir='/home/ec2-user/SageMaker/Data/entrel_data_con_eval_new'\n",
    "    args.output_dir='/tmp/entrel/' + str(networka)\n",
    "    #\n",
    "    results = {}\n",
    "    MODEL_CLASSES = {\n",
    "        'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "        'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
    "        'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
    "        'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
    "        'distilbert': (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer)\n",
    "    }\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        args.n_gpu = torch.cuda.device_count()\n",
    "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "        args.n_gpu = 1\n",
    "    #\n",
    "    args.device = device\n",
    "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "    tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "    checkpoints = [args.output_dir]\n",
    "    checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True)))\n",
    "    args.output_mode = output_modes[args.task_name]\n",
    "    checkpoint=checkpoints[-1]\n",
    "    global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
    "    prefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else \"\"\n",
    "    model = model_class.from_pretrained(checkpoint)\n",
    "    model.to(args.device)\n",
    "    #result = run_glue_gap.evaluate(args, model, tokenizer, prefix=prefix)\n",
    "    eval_task=args.task_name\n",
    "    eval_dataset = run_glue_gap.load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    start_time=time.time()\n",
    "    #for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    for batch in eval_dataloader:\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels':         batch[3]}\n",
    "            if args.model_type != 'distilbert':\n",
    "                inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None  # XLM, DistilBERT and RoBERTa don't use segment_ids\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "    #\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    final_preds = np.argmax(preds, axis=1)\n",
    "    result = simple_accuracy(final_preds, out_label_ids)\n",
    "    nupreds=[list(map(float,list(preds[:,0]))),list(map(float, list(preds[:,1])))]\n",
    "    all_preds.append(nupreds)\n",
    "    all_finalpreds.append(list(map(float,list(final_preds))))\n",
    "    all_actuals.append(list(map(float,list(out_label_ids))))\n",
    "    all_accs.append(result)\n",
    "    all_times.append(time.time()-start_time)\n",
    "\n",
    "\n",
    "the_dic={'all_outputs':all_preds,'all_preds':all_finalpreds,'all_actuals':all_actuals,'all_accs':all_accs,'all_times':all_times}\n",
    "filename='/home/ec2-user/SageMaker/Data/roberta_testdata_outputs_new.json'\n",
    "with open(filename, 'w') as outfile:\n",
    "    json.dump(the_dic, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final test set on 1 network:\n",
    "# XLNET\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import run_glue_gap\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,TensorDataset)\n",
    "import tqdm\n",
    "from transformers import (WEIGHTS_NAME, BertConfig,\n",
    "                                  BertForSequenceClassification, BertTokenizer,\n",
    "                                  RobertaConfig,\n",
    "                                  RobertaForSequenceClassification,\n",
    "                                  RobertaTokenizer,\n",
    "                                  XLMConfig, XLMForSequenceClassification,\n",
    "                                  XLMTokenizer, XLNetConfig,\n",
    "                                  XLNetForSequenceClassification,\n",
    "                                  XLNetTokenizer,\n",
    "                                  DistilBertConfig,\n",
    "                                  DistilBertForSequenceClassification,\n",
    "                                  DistilBertTokenizer)\n",
    "from utils_glue_gap import (compute_metrics,\n",
    "                        output_modes, processors, simple_accuracy)\n",
    "#\n",
    "all_preds=[]\n",
    "all_finalpreds=[]\n",
    "all_actuals=[]\n",
    "all_accs=[]\n",
    "all_times=[]\n",
    "#networka=0\n",
    "#\n",
    "parser = run_glue_gap.argparse.ArgumentParser()\n",
    "args = run_glue_gap.get_args(parser)\n",
    "#\n",
    "model_typea=1 # 0: BERT, 1: XLNET, 2: Roberta, 3: Distilbert, 4: XLM, 5:TransformerXL\n",
    "small_or_big=0 # 0: small, 1: big\n",
    "args.con_or_lib=0 # 0: con, 1: lib, 2: original\n",
    "batch_size=16\n",
    "#\n",
    "args.num_train_epochs=9\n",
    "args.do_train=True\n",
    "args.do_eval=True\n",
    "#\n",
    "#args.max_steps=3\n",
    "args.overwrite_output_dir=True\n",
    "args.overwrite_cache = True\n",
    "args.evaluate_during_training=False\n",
    "args.logging_steps =50\n",
    "args.save_steps=1000\n",
    "args.max_seq_length = 512\n",
    "args.seed=round(time.time())\n",
    "#\n",
    "args.learning_rate =7.5e-6\n",
    "args.adam_epsilon=1e-8\n",
    "args.warmup_steps= 97  \n",
    "args.weight_decay= 0.0\n",
    "#\n",
    "if small_or_big==0:\n",
    "    args.per_gpu_eval_batch_size=8\n",
    "    args.per_gpu_train_batch_size=8\n",
    "    if batch_size==8:\n",
    "        args.gradient_accumulation_steps=1\n",
    "    elif batch_size==16:\n",
    "        args.gradient_accumulation_steps=2\n",
    "    elif batch_size==32:\n",
    "        args.gradient_accumulation_steps=4\n",
    "elif small_or_big==1:\n",
    "    args.per_gpu_eval_batch_size=1\n",
    "    args.per_gpu_train_batch_size=1\n",
    "    if batch_size==8:\n",
    "        args.gradient_accumulation_steps=8\n",
    "    elif batch_size==16:\n",
    "        args.gradient_accumulation_steps=16\n",
    "    elif batch_size==32:\n",
    "        args.gradient_accumulation_steps=32\n",
    "#\n",
    "#\n",
    "if model_typea==0:\n",
    "    args.model_type='bert'\n",
    "    args.do_lower_case=True\n",
    "    if small_or_big==0:\n",
    "        args.model_name_or_path='bert-base-uncased'\n",
    "    else:\n",
    "        args.model_name_or_path='bert-large-uncased'\n",
    "elif model_typea==1:\n",
    "    args.model_type='xlnet'\n",
    "    args.do_lower_case=False\n",
    "    #args.model_name_or_path='xlnet-base-cased'\n",
    "    if small_or_big==0:\n",
    "        args.model_name_or_path='xlnet-base-cased'\n",
    "    else:\n",
    "        args.model_name_or_path='xlnet-large-cased'\n",
    "elif model_typea==2:\n",
    "    args.model_type='roberta'\n",
    "    args.do_lower_case=False\n",
    "    #args.model_name_or_path='roberta-base'\n",
    "    if small_or_big==0:\n",
    "        args.model_name_or_path='roberta-base'\n",
    "    else:\n",
    "        args.model_name_or_path='roberta-large'\n",
    "elif model_typea==3:\n",
    "    args.model_type='distilbert'\n",
    "    args.model_name_or_path='distilbert-base-uncased'\n",
    "    args.do_lower_case=True\n",
    "elif model_typea==4:\n",
    "    args.model_type='xlm'\n",
    "    args.model_name_or_path='xlm-mlm-en-2048'\n",
    "    args.do_lower_case=False\n",
    "#\n",
    "#\n",
    "args.task_name='entityrel'\n",
    "args.data_dir='/home/ec2-user/SageMaker/Data/entrel_data_con_eval2'\n",
    "args.output_dir='/tmp/entrel/'\n",
    "#\n",
    "results = {}\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "    'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
    "    'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
    "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
    "    'distilbert': (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer)\n",
    "}\n",
    "# Setup CUDA, GPU & distributed training\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "    args.n_gpu = 1\n",
    "#\n",
    "args.device = device\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "checkpoints = [args.output_dir]\n",
    "checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True)))\n",
    "args.output_mode = output_modes[args.task_name]\n",
    "checkpoint=checkpoints[-1]\n",
    "global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
    "prefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else \"\"\n",
    "model = model_class.from_pretrained(checkpoint)\n",
    "model.to(args.device)\n",
    "#result = run_glue_gap.evaluate(args, model, tokenizer, prefix=prefix)\n",
    "eval_task=args.task_name\n",
    "eval_dataset = run_glue_gap.load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n",
    "args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "eval_loss = 0.0\n",
    "nb_eval_steps = 0\n",
    "preds = None\n",
    "out_label_ids = None\n",
    "start_time=time.time()\n",
    "#for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "for batch in eval_dataloader:\n",
    "    model.eval()\n",
    "    batch = tuple(t.to(args.device) for t in batch)\n",
    "    with torch.no_grad():\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[3]}\n",
    "        if args.model_type != 'distilbert':\n",
    "            inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None  # XLM, DistilBERT and RoBERTa don't use segment_ids\n",
    "        outputs = model(**inputs)\n",
    "        tmp_eval_loss, logits = outputs[:2]\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "    nb_eval_steps += 1\n",
    "    if preds is None:\n",
    "        preds = logits.detach().cpu().numpy()\n",
    "        out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "    else:\n",
    "        preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "        out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "#\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "final_preds = np.argmax(preds, axis=1)\n",
    "result = simple_accuracy(final_preds, out_label_ids)\n",
    "nupreds=[list(map(float,list(preds[:,0]))),list(map(float, list(preds[:,1])))]\n",
    "all_preds.append(nupreds)\n",
    "all_finalpreds.append(list(map(float,list(final_preds))))\n",
    "all_actuals.append(list(map(float,list(out_label_ids))))\n",
    "all_accs.append(result)\n",
    "all_times.append(time.time()-start_time)\n",
    "\n",
    "\n",
    "the_dic={'all_outputs':all_preds,'all_preds':all_finalpreds,'all_actuals':all_actuals,'all_accs':all_accs,'all_times':all_times}\n",
    "filename='/home/ec2-user/SageMaker/Data/xlnet_testdata_nooverlap_outputs.json'\n",
    "with open(filename, 'w') as outfile:\n",
    "    json.dump(the_dic, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final test set on 1 network:\n",
    "# any model\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import run_glue_gap\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,TensorDataset)\n",
    "import tqdm\n",
    "import sklearn\n",
    "from transformers import (WEIGHTS_NAME, BertConfig,\n",
    "                                  BertForSequenceClassification, BertTokenizer,\n",
    "                                  RobertaConfig,\n",
    "                                  RobertaForSequenceClassification,\n",
    "                                  RobertaTokenizer,\n",
    "                                  XLMConfig, XLMForSequenceClassification,\n",
    "                                  XLMTokenizer, XLNetConfig,\n",
    "                                  XLNetForSequenceClassification,\n",
    "                                  XLNetTokenizer,\n",
    "                                  DistilBertConfig,\n",
    "                                  DistilBertForSequenceClassification,\n",
    "                                  DistilBertTokenizer)\n",
    "from utils_glue_gap import (compute_metrics,\n",
    "                        output_modes, processors, simple_accuracy)\n",
    "#\n",
    "all_preds=[]\n",
    "all_finalpreds=[]\n",
    "all_actuals=[]\n",
    "all_accs=[]\n",
    "all_f1s=[]\n",
    "all_times=[]\n",
    "all_counts=[]\n",
    "#networka=0\n",
    "#\n",
    "parser = run_glue_gap.argparse.ArgumentParser()\n",
    "args = run_glue_gap.get_args(parser)\n",
    "#\n",
    "model_typea=2 # 0: BERT, 1: XLNET, 2: Roberta, 3: Distilbert, 4: XLM, 5:TransformerXL\n",
    "small_or_big=0 # 0: small, 1: big\n",
    "args.con_or_lib=0 # 0: con, 1: lib, 2: original\n",
    "batch_size=32\n",
    "#\n",
    "args.num_train_epochs=5.0\n",
    "args.do_train=True\n",
    "args.do_eval=True\n",
    "#\n",
    "#args.max_steps=3\n",
    "args.overwrite_output_dir=True\n",
    "args.overwrite_cache = True\n",
    "args.evaluate_during_training=False\n",
    "args.logging_steps =50\n",
    "args.save_steps=1000\n",
    "args.max_seq_length = 512\n",
    "args.seed=round(time.time())\n",
    "#\n",
    "args.learning_rate =6.3e-5\n",
    "args.adam_epsilon=1e-8\n",
    "args.warmup_steps= 97  \n",
    "args.weight_decay= 0.0\n",
    "#\n",
    "if small_or_big==0:\n",
    "    args.per_gpu_eval_batch_size=8\n",
    "    args.per_gpu_train_batch_size=8\n",
    "    if batch_size==8:\n",
    "        args.gradient_accumulation_steps=1\n",
    "    elif batch_size==16:\n",
    "        args.gradient_accumulation_steps=2\n",
    "    elif batch_size==32:\n",
    "        args.gradient_accumulation_steps=4\n",
    "elif small_or_big==1:\n",
    "    args.per_gpu_eval_batch_size=1\n",
    "    args.per_gpu_train_batch_size=1\n",
    "    if batch_size==8:\n",
    "        args.gradient_accumulation_steps=8\n",
    "    elif batch_size==16:\n",
    "        args.gradient_accumulation_steps=16\n",
    "    elif batch_size==32:\n",
    "        args.gradient_accumulation_steps=32\n",
    "\n",
    "\n",
    "if model_typea==0:\n",
    "    args.model_type='bert'\n",
    "    args.do_lower_case=True\n",
    "    if small_or_big==0:\n",
    "        args.model_name_or_path='bert-base-uncased'\n",
    "    else:\n",
    "        args.model_name_or_path='bert-large-uncased'\n",
    "elif model_typea==1:\n",
    "    args.model_type='xlnet'\n",
    "    args.do_lower_case=False\n",
    "    #args.model_name_or_path='xlnet-base-cased'\n",
    "    if small_or_big==0:\n",
    "        args.model_name_or_path='xlnet-base-cased'\n",
    "    else:\n",
    "        args.model_name_or_path='xlnet-large-cased'\n",
    "elif model_typea==2:\n",
    "    args.model_type='roberta'\n",
    "    args.do_lower_case=False\n",
    "    #args.model_name_or_path='roberta-base'\n",
    "    if small_or_big==0:\n",
    "        args.model_name_or_path='roberta-base'\n",
    "    else:\n",
    "        args.model_name_or_path='roberta-large'\n",
    "elif model_typea==3:\n",
    "    args.model_type='distilbert'\n",
    "    args.model_name_or_path='distilbert-base-uncased'\n",
    "    args.do_lower_case=True\n",
    "elif model_typea==4:\n",
    "    args.model_type='xlm'\n",
    "    args.model_name_or_path='xlm-mlm-en-2048'\n",
    "    args.do_lower_case=False\n",
    "\n",
    "\n",
    "args.task_name='entityrel'\n",
    "#args.data_dir='/home/ec2-user/SageMaker/Data/entrel_data_con_eval_norm'\n",
    "#args.data_dir='/home/ec2-user/SageMaker/Data/entrel_data_con_eval_MT_test_new'\n",
    "args.data_dir='/home/ec2-user/SageMaker/Data/entrel_data_con_eval_aetna_new'\n",
    "#args.data_dir='/home/ec2-user/SageMaker/Data/entrel_temp' #entrel_data_con train is saved in dev\n",
    "#args.data_dir='/home/ec2-user/SageMaker/Data/entrel_data_con_eval_new2'\n",
    "#args.data_dir='/home/projects/data/entrel_data_con_eval_new'\n",
    "args.output_dir='/tmp/entrel/'\n",
    "#\n",
    "results = {}\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "    'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
    "    'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
    "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
    "    'distilbert': (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer)\n",
    "}\n",
    "# Setup CUDA, GPU & distributed training\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "    args.n_gpu = 1\n",
    "\n",
    "    \n",
    "args.device = device\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "checkpoints = [args.output_dir]\n",
    "checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True)))\n",
    "args.output_mode = output_modes[args.task_name]\n",
    "checkpoint=checkpoints[-1]\n",
    "global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
    "prefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else \"\"\n",
    "model = model_class.from_pretrained(checkpoint)\n",
    "model.to(args.device)\n",
    "#result = run_glue_gap.evaluate(args, model, tokenizer, prefix=prefix)\n",
    "eval_task=args.task_name\n",
    "eval_dataset = run_glue_gap.load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n",
    "args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "eval_loss = 0.0\n",
    "nb_eval_steps = 0\n",
    "preds = None\n",
    "out_label_ids = None\n",
    "start_time=time.time()\n",
    "#for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "for batch in eval_dataloader:\n",
    "    model.eval()\n",
    "    batch = tuple(t.to(args.device) for t in batch)\n",
    "    with torch.no_grad():\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[3]}\n",
    "        if args.model_type != 'distilbert':\n",
    "            inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None  # XLM, DistilBERT and RoBERTa don't use segment_ids\n",
    "        outputs = model(**inputs)\n",
    "        tmp_eval_loss, logits = outputs[:2]\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "    nb_eval_steps += 1\n",
    "    if preds is None:\n",
    "        preds = logits.detach().cpu().numpy()\n",
    "        out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "    else:\n",
    "        preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "        out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "final_preds = np.argmax(preds, axis=1)\n",
    "result = simple_accuracy(final_preds, out_label_ids)\n",
    "nupreds=[list(map(float,list(preds[:,0]))),list(map(float, list(preds[:,1])))]\n",
    "all_preds.append(nupreds)\n",
    "all_finalpreds.append(list(map(float,list(final_preds))))\n",
    "all_actuals.append(list(map(float,list(out_label_ids))))\n",
    "all_accs.append(result)\n",
    "all_f1s.append(sklearn.metrics.f1_score(all_actuals[-1],all_finalpreds[-1],average='macro'))\n",
    "TPs=[inda for inda,bla in enumerate(all_actuals[-1]) if (bla==1.0 and all_finalpreds[-1][inda]==1.0)]\n",
    "FNs=[inda for inda,bla in enumerate(all_actuals[-1]) if (bla==1.0 and all_finalpreds[-1][inda]==0.0)]\n",
    "TNs=[inda for inda,bla in enumerate(all_actuals[-1]) if (bla==0.0 and all_finalpreds[-1][inda]==0.0)]\n",
    "FPs=[inda for inda,bla in enumerate(all_actuals[-1]) if (bla==0.0 and all_finalpreds[-1][inda]==1.0)]\n",
    "all_counts.append((len(TPs),len(TNs),len(FPs),len(FNs)))\n",
    "all_times.append(time.time()-start_time)\n",
    "#sklearn.metrics.f1_score( list(map(float,list(out_label_ids))), list(map(float,list(preds))), average='macro')\n",
    "\n",
    "the_dic={'all_outputs':all_preds,'all_preds':all_finalpreds,'all_actuals':all_actuals,'all_accs':all_accs, \\\n",
    "         'all_times':all_times, 'all_f1s':all_f1s, 'all_counts':all_counts}\n",
    "filename='/home/ec2-user/SageMaker/Data/roberta_testdata_justone_outputs_norm.json'\n",
    "with open(filename, 'w') as outfile:\n",
    "    json.dump(the_dic, outfile)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
